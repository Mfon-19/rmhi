{
  "scraped_ideas": [
    {
      "project_name": "Smart Fridge",
      "likes": 91,
      "submitted_to": "Hacktech",
      "winner": true,
      "created_by": "https://devpost.com/YeLing7",
      "description": "A Smart Fridge that uses Computer Vision to log in food, keeps user updated by SMS, and provide recommendations.\n\nWe saw the brand new Samsung Family Hub smart fridge at the CES 2017, which require manual data log-in for the goods stored inside. We got inspired to create a smart fridge that can automatically log in what's inside the fridge, enable users to access the data remotely and have information recommended for the users based on what they have in the fridge.\n\nThis is an IoT-based smart fridge that uses Computer Vision to automatically log in food, informs the users through text messages of what's stored inside and expiration data, and recommend healthier and better use of user's’ current storage through features like checking nutrition and search for recipes related to some items.\n\nWe used a button on an Arduino board to emulate the action of “closing the fridge door”. The signal created by the button is sent to a PC through a serial COM port. When PC receives that signal, the kinect camera is triggered to capture a photo of the current status in the fridge. The photo is then compressed and sent to our web server. Our web server is coded on Python+Flask and deployed on Google App Engine Flexible Environment. This web server also contains some logics for responding to Twilio messages, which will be mentioned later. When the web server receives that photo, it puts the photo in Google Cloud Storage. It also keeps some basic image metadata in Google Cloud Datastore database. Then the Google Cloud Vision API is called to analyze the photo and label it by what the item is and which category it belongs to. The labels (coming out of cloud vision api) are then passed to Google KnowledgeGraph API to be further narrowed down to things people would normally put in a fridge. The results coming out of Google KnowledgeGraph are then stored in Google Cloud Datastore database. Now the fridge basically identifies the items that were put in it by automatically capturing and analyzing photos. Every time new items are added to the fridge, Twilio would send a notification through SMS to inform user\nUsers are also able to text Twilio some basic commands to:\nCheck what is currently in the fridge\nCheck which item is about to pass its expiration date\nCheck the nutrition of the food stored\nSearch for recipes related to some items",
      "technologies": [
        "arduino",
        "c++",
        "google-app-engine",
        "google-cloud",
        "google-cloud-datastore",
        "google-cloud-vision",
        "google-knowledgegraph",
        "google-ml",
        "kinect",
        "python",
        "twilio",
        "wolfram-technologies"
      ]
    },
    {
      "project_name": "Chromelexa",
      "likes": 28,
      "submitted_to": "HackUPC Winter 2017",
      "winner": false,
      "created_by": "https://devpost.com/mbell",
      "description": "Control your browser with your Alexa device.\n\nWe wanted to do something interesting with Alexa, so we decided to make a skill. We noticed that while you can use Alexa to stream movies and music to your TV, you can't control your computer's laptop with it. So we decided to make this system!\n\nUsing your Echo/other Alexa, load the skill and speak:\nYou get the drill. An extension installed on your Chrome browser performs the actions you give it.\n\nThe system uses four main parts: An Alexa skill, an AWS Lambda function that handles the skill, a webserver, and a Chrome extension.\nThe Alexa skill simply allows the user to dictate commands, and the Lambda function processes these. The function then sends the command to the webserver (in a HTTP POST request). This webserver runs a Socket.io server, and when it receives a command from the Lambda function, it emits an event to the Chrome extension containing the command. The extension receives this event, processes the command, then runs it on the current browser tab.",
      "technologies": [
        "alexa",
        "amazon-web-services",
        "chrome",
        "lambda",
        "node.js",
        "scalingo",
        "socket.io"
      ]
    },
    {
      "project_name": "HandWashMonitor",
      "likes": 29,
      "submitted_to": "GE Health Cloud Innovation Challenge",
      "winner": false,
      "created_by": "https://devpost.com/pradeepsaiu",
      "description": "To maintain hygiene of hands in hospitals or restaurants, Smart watch on the hand can monitor how clean the handsare\n\nHand cleanliness is minimum basic requirement, largely not neglected but forgotten by many. Could be professionals in hospitals or Restaurants.\n\nBased on the accelerometer and gyroscope values of smart watch, It ensures that the WHO standards for hand wash are performed, when person is coming out of unhygienic place or entering a place which should be maintained neatly.",
      "technologies": [
        "android-studio",
        "android-wear",
        "bluetooth",
        "matlab"
      ]
    },
    {
      "project_name": "Center Scope",
      "likes": 15,
      "submitted_to": "DerbyHacks 2",
      "winner": false,
      "created_by": "https://devpost.com/csinko",
      "description": "Perfect Telescope Calibration Using Image Processing\n\nThe idea came to me from frustration with trying to set up / calibrate my telescope.  It was a slow and painful process and I knew that it could be sped up with a computer.\n\nA USB Camera is connected to the telescope along with a serial connection to a computer.  Once a star is in the general view, the user can select the object and the telescope will automatically center it perfectly, which then allows you to view the object dead on with a high quality lens.\n\nThe telescope does NOT have a serial port to connect to a computer.  Because of this, we had to reverse-engineer the communication between the controller and telescope.  The initialization codes movement codes, and stop codes were found and converted / moved into a python program to be able to control the telescope from a computer.\nThe program will use OpenCV to add a threshold to the image from the camera.  This will rule out any objects that are not bright.  It will then find all of the objects and when the desired one is chosen, the serial commands are sent to move the telescope to get it centered.",
      "technologies": [
        "binary",
        "opencv",
        "python"
      ]
    },
    {
      "project_name": "Holdup",
      "likes": 21,
      "submitted_to": "HackISU Spring 2017",
      "winner": false,
      "created_by": "https://devpost.com/nathangitter",
      "description": "Send Gamified Messages to Your Friends!\n\nFun new ways to communicate with friends are growing in popularity, and we wanted to build a communication tool of our own. We took inspiration from the \"money maze\" toy, which locks a gift in a transparent cube maze. In order to unlock the cube and get the gift inside, the recipient must complete a complex puzzle.\n\nHoldup is an iMessage app that allows users to send secret messages and payments. The catch is that in order to read the message and accept the payment, the recipient must complete a challenging mini-game. The game uses the device's accelerometer, and is intended to frustrate users who want to read the message or receive their money. Senders can choose from three difficulty levels, and a unique game level is automatically generated. The communication is built directly into the iMessage platform, which allows users to send the mini-game to their friends, even if they don't have the app installed.\n\nThe app is written in Swift and built with Xcode. It exists almost exclusively as an iMessage extension. The mini-game uses SpriteKit, a 2D game engine, to render sprites, simulate physics, and detect collisions.",
      "technologies": [
        "adobe-illustrator",
        "imessage",
        "ios",
        "sketch",
        "sprite-kit",
        "swift",
        "venmo",
        "xcode"
      ]
    },
    {
      "project_name": "Shoebotics",
      "likes": 21,
      "submitted_to": "HackISU Spring 2017",
      "winner": true,
      "created_by": "https://devpost.com/galval28",
      "description": "A personal following cargo robot for all of your personal following cargo robot needs.\n\nWe wanted to incorporate our passion for autonomous robotics with wireless communication, serving the niche for travelling assistance in an increasingly  world.\n\nCargo robot that connects to a radio transmitter attached to the user (currently integrated in a shoe), using sensor data and programmed logic to follow the user in potentially any environment.\n\nWe built it in multiple modules, separating the data receiver (robot communications), hardware control (robot outputs), and data transmission (shoe). After each was developed and tested individually, we began the long - and somewhat tedious - process of integration.",
      "technologies": [
        "arduino",
        "c++",
        "home-made-motor-drivers",
        "nrf-transceivers"
      ]
    },
    {
      "project_name": "Raw Manga Translator (3A)",
      "likes": 62,
      "submitted_to": "BrickHack 3",
      "winner": false,
      "created_by": "",
      "description": "Sick of waiting for manga with translated text to come out?Well, you've come to a right place!\n\nOne early Wednesday morning, I woke up excitedly to read a new chapter on One Piece. However, there was no translated text yet. With anger, frustration, bargaining, depression, and acceptance, we decided to start this project to prevent this from happening again.\n\nAs of now, it asks for an image URL (specifically a link to one of the chapter's image in some manga) and the program will show you what text are on the image, then the text will be translated afterward.\n\nPython 3\nMicrosoft's Cognitive Services (We used their Computer Vision API to extract the text from the image)\nGoogle Translate API",
      "technologies": [
        "google-translate-api",
        "microsoft-computer-vision-api",
        "python"
      ]
    },
    {
      "project_name": "NotificationsApp",
      "likes": 24,
      "submitted_to": "Pearl Hacks 2017",
      "winner": true,
      "created_by": "https://devpost.com/jenniferqian",
      "description": "Has your phone ever annoyed you by ringing incessantly from a tirade of notifications? This app fixes that!\n\nHas your phone ever annoyed you by ringing incessantly from a tirade of notifications you don't even care about that much? Annoying group messages from projects?\n\nThis app will let you select messaging apps that you want to block notifications when the frequency of notifications exceeds 1 every 3 seconds, allowing you to receive messages without the annoying train of notifications.",
      "technologies": [
        "java"
      ]
    },
    {
      "project_name": "ResistAR",
      "likes": 41,
      "submitted_to": "TartanHacks 2017",
      "winner": true,
      "created_by": "https://devpost.com/liuwilliam47",
      "description": "Augmented Reality Circuit Visualizer and Solver\n\nLong hours spent on ECE problem sets and frustration visualizing convoluted circuits caused these four CMU undergrads to create a circuit visualization system that would also help them solve circuits.\nA member of the team is currently in the intro ECE course: \"Well it's not bad, I guess.\" - Team Member\n\nResistAR is an Augmented Reality Circuit Visualizer and Solver. A user can place down circuit elements in parallel and series configurations and ResistAR will solve the current through and voltage across each element of the circuit. It gives the user an easy way to see (sharp) the circuit.\n\nWe first began with 3D printed chassis for the VuMark targets. These targets are identified and parsed by the program and cross checked against our cloud database on Vuforia. \nWe then created 3D, textured, models in Blender that will hover over the VuMark targets.\nWe then wrote the code in Unity that will calculate voltage and current values using concepts from vector calculus and matrix algebra.",
      "technologies": [
        "3d-printer",
        "blender",
        "c#",
        "matrix-algebra",
        "solidworks",
        "unity",
        "vector-calculus"
      ]
    },
    {
      "project_name": "Holla!",
      "likes": 135,
      "submitted_to": "Hackatown",
      "winner": false,
      "created_by": "https://devpost.com/BernardSGS",
      "description": "Human have short term memory, so we always disappoint by forgetting to text. Holla! takes care of that!\n\nHow many time have you forgotten to text your mom to let her know that you had reached home safely after volley-ball practice? How many times have you also forgotten to text Jennifer that you had got home in one piece after a wild night out ? Too many times! But now, we're fixing it!\n\nEnables you to automatically send a text message to your friend, family or loved one when you get to a specified location. Even if you forget. Ensuring your friend Bobby always gets home in one piece, one text at a time.\n\nNative android development, built with android studio.",
      "technologies": [
        "adobe-illustrator",
        "android",
        "google-maps",
        "love",
        "photoshop"
      ]
    },
    {
      "project_name": "eyeBeacon",
      "likes": 28,
      "submitted_to": "Hackatown",
      "winner": false,
      "created_by": "https://devpost.com/woozzie",
      "description": "To help the visually impaired travel across the city safer and smarter.\n\nTo help the visually impaired cross the street and making the city smarter. #smartcity\n\neyeBeacon is a phone application where the user is able to walk around freely and get notified (by speech or by different hand gestures) whenever they get near or towards a Beacon (Estimotes).\n\nOur android application was built with Android Studio. We were able to connect the Estimote Beacons through integrating their SDK and dependencies into our system. We integrated text to speech to output voice commands.",
      "technologies": [
        "android-studio",
        "beacons",
        "estimote",
        "java",
        "photoshop",
        "xml"
      ]
    },
    {
      "project_name": "TwoSidedNews",
      "likes": 22,
      "submitted_to": "Hack@Brown 2017",
      "winner": true,
      "created_by": "https://devpost.com/laurho",
      "description": "Juxtaposition of sources from opposite sides of the political spectrum\n\nThe primary inspiration of our project is the growing filter bubbles in our country and in our world. The ability of people to only see posts on social media and news sites that agree with their point of view is a worrying development of the 21st century that we are trying to combat. Our project being two-fold (Chrome Extension and website) means that we can both provide an in depth overview of an issue for those who are actively curious about learning more about an issue through our website, and also passively prompt users with alternative interpretations of news stories for users who are not consistently conscious of filter bubbles.\n\nTwo Sided News attempts to give two views on any story: liberal and conservative. By searching keywords to a topic the user would like to read about, our website displays articles side by side. The user can then choose to read whichever perspective they please, or both, and come up with their own interpretations of the story.\nThe chrome extension version allows you to directly look at another article from the opposite view. This way, users can continue to browse articles on websites that they are comfortable with, but have the option to read the another side of the story through this extension.\n\nWe used HTML, CSS, and JavaScript (all the classic web development basics!) for creating our website. We hosted through firebase, and we also used JavaScript for the Chrome extension. The querying was handled with a custom google search engine that searched for articles from selected sources.",
      "technologies": [
        "css",
        "google-cse-api",
        "html",
        "javascript",
        "jquery"
      ]
    },
    {
      "project_name": "Selfie Activism",
      "likes": 15,
      "submitted_to": "DevFest 2017",
      "winner": true,
      "created_by": "https://devpost.com/spencersyen",
      "description": "Choose an issue you care about. Take a selfie. We mail a postcard with your face to your local representative.\n\nYoung people aren't communicating with their government and are not aware of political issues. We help Selfie Activism will inspire them to become more politically active.\n\nPick an issue you're interested in\nTake a selfie for the postcard\nWrite a message on your stance on the issue\nUse your location to find your local representatives\nWe send your postcard (physical!) to them using the Lob API!\n\nWe made it in Objective-C with Lob API to send the postcard. We scrape issues and their descriptions from 5calls.org",
      "technologies": []
    },
    {
      "project_name": "Yoogle",
      "likes": 61,
      "submitted_to": "McHacks 2017",
      "winner": false,
      "created_by": "https://devpost.com/SokHengLim",
      "description": "The web application that can search text on the content of video files and direct you to the portion of the video\n\nWe got into a discussion about YouTube videos and how it would be a good idea to have a Ctrl-F capability for audio.\n\nThrough an Web application interface, users can type the word and the video they want to search. It finds occurrences of the word in an XML file of a YouTube video transcript and returns the times it occurs. The video is then viewable on the webpage and the user can skip the video to the time they want.\n\nWe used Python to build our app backend and CSS, HTML and JavaScript for our UI aspect",
      "technologies": [
        "bootstrap",
        "chrome",
        "css",
        "flask",
        "javascript",
        "python",
        "selenium",
        "youtube"
      ]
    },
    {
      "project_name": "AirBnBro",
      "likes": 34,
      "submitted_to": "McHacks 2017",
      "winner": true,
      "created_by": "https://devpost.com/ruoyutao",
      "description": "The bro that will help you find your next AirBnB!\n\nMy friend and I needed to find an apartment in New York City during the Summer. We found it very difficult to look through multiple listing pages at once so we thought to make a bot to suggest apartments would be helpful. However, we did not stop there. We realized that we could also use Machine Learning so the bot would learn what we like and suggest better apartments. That is why we decided to do RealtyAI\n\nIt is a facebook messenger bot that allows people to search through airbnb listings while learning what each user wants. By giving feedback to the bot, we learn your general style and thus we are able to recommend the apartments that you are going to like, under your budget, in any city of the world :) We can also book the apartment for you.",
      "technologies": [
        "chatbot",
        "machine-learning",
        "naive-bayes",
        "python",
        "sklearn"
      ]
    },
    {
      "project_name": "Gfytti",
      "likes": 16,
      "submitted_to": "GIF Hack",
      "winner": true,
      "created_by": "https://devpost.com/huyle333",
      "description": "We've seen gfycat on computers, we've seen gfycat on our phones, and now we can use gfycat in mixed reality.\n\nCreate an immersive way to experience and interact with gfycats in your environment.\n\nGfycats in AR. Search for gfycats and place them in the environment. Geolocation added to gfycat so you can see gfycats placed in the same area by other users.\n\nUnity, C#, Hololens Toolkit",
      "technologies": [
        "c#",
        "microsoft-hololens",
        "unity"
      ]
    },
    {
      "project_name": "null",
      "likes": 12,
      "submitted_to": "MinneHack",
      "winner": false,
      "created_by": "https://devpost.com/Andrew-Casner",
      "description": "null",
      "technologies": []
    },
    {
      "project_name": "060 - iTrash",
      "likes": 133,
      "submitted_to": "Hack&Roll 2017",
      "winner": false,
      "created_by": "https://devpost.com/cflee",
      "description": "encouraging recycling with smart image-recognising dustbins\n\nRecycling rate in Singapore is quite low, with plastics at a mere 7%. Even if people know what materials are recyclable, they often don't have the habit of placing those items into recycling instead. Let's use technology to train people to build this habit!\n\nThe smart dustbin visually inspects the items, and alerts the user if the object is recyclable. This event is logged too to see how much trash has been spotted and (hopefully) diverted to the recycling stream.\n\nHardware: A Raspberry Pi with camera snaps a photo periodically and sends it to the backend, and if it determines that the item is recyclable, a visual alert is displayed on the OLED screen.\nBackend: The Python/Flask server first sends the photo to Microsoft Cognitive Services' Computer Vision API for analysis, which returns a textual description of the image. If that result is not conclusive, then the image is sent to a convolutional neural network (GoogLeNet) built on the Caffe library that has been pre-trained to do material classification.\nDatabase: The backend logs successful recognitions to an Elasticsearch database, including the type of recycled material seen and the timestamp.\nFrontend: Kibana is used to serve a dynamic dashboard.",
      "technologies": [
        "caffe",
        "cognitive-vision-api",
        "deep-learning",
        "elasticsearch",
        "flask",
        "googlenet",
        "kibana",
        "python",
        "raspberry-pi"
      ]
    },
    {
      "project_name": "HackTube",
      "likes": 35,
      "submitted_to": "SB Hacks III",
      "winner": true,
      "created_by": "https://devpost.com/nishelat",
      "description": "A Chrome extension that fights online harassment by filtering out comments with strong language.\n\nYouTube is a place for millions of people to share their voices and engage with their communities. Unfortunately, the YouTube comments section is notorious for enabling anonymous users to post hateful and derogatory messages with the click of a button. These messages are purely meant to cause anger and depression without ever providing any constructive criticism. For YouTubers, this means seeing the degrading and mentally-harmful comments on their content, and for the YouTube community, this means reading negative and offensive comments on their favorite videos. As young adults who consume this online content, we feel as though it is necessary to have a tool that combats these comments to make YouTube a safer place.\n\nHackTube automatically analyzes every YouTube video you watch, targeting comments which are degrading and offensive. It is constantly checking the page for hateful comments, so if the user loads more comments, the extension will pick those up. It then blocks comments which it deems damaging to the user, listing the total number of blocked comments at the top of the page. This process is all based on user preference, since the user chooses which types of comments (sexist, racist, homophobic, etc) they do not want to see. It is important to note that the user can disable the effects of the extension at any time. HackTube is not meant to censor constructive criticism; rather, it combats comments which are purely malicious in intent.\n\nHackTube uses JavaScript to parse through every YouTube comment almost instantly, comparing its content to large arrays that we made which are full of words that are commonly used in hate speech. We chose our lists of words carefully to ensure that the extension would focus on injurious comments rather than helpful criticism. We used standard HTML and CSS to style the popup for the extension and the format of the censored comments.",
      "technologies": [
        "adobe-illustrator",
        "css",
        "html",
        "javascript",
        "json"
      ]
    },
    {
      "project_name": "Arduino Integrated Dumbbell",
      "likes": 12,
      "submitted_to": "HackDavis 2017",
      "winner": false,
      "created_by": "https://devpost.com/mintypaladin",
      "description": "The dumbbell that's not dumb! Rep counting has never been easier!",
      "technologies": [
        "arduino"
      ]
    },
    {
      "project_name": "Go Alexa Go",
      "likes": 15,
      "submitted_to": "SpartaHack 2017",
      "winner": true,
      "created_by": "https://devpost.com/RandyLee",
      "description": "GO ALEXA GO was created with millennials in mind. Why drive and text when you can just text and have Alexa drive?\n\nWe were inspired by Millennials' dangerous texting and driving habits, so we developed a driving system to allow them to text and still drive at the same time.\n\nOur HTC Vive virtual reality experience allows the user to issue commands to our taxi driver, Alexa, and explore Sponsorville.\n\nWe built our HTC Vive VR experience in Unity using C# and our Amazon backend with node.js and the Alexa skillset. The Amazon Alexa is able to take a user's directional input voice command through Amazon's unique browser-based web services built with node.js, and notifies Unity of the user's input with a web API hosted on Microsoft Azure.",
      "technologies": [
        "adobe-dreamweaver",
        "adobe-illustrator",
        "agile",
        "amazon-alexa",
        "amazon-web-services",
        "asp.net",
        "azure",
        "c#",
        "htc-vive",
        "iot",
        "javascript",
        "node.js",
        "photoshop",
        "redbull",
        "unity"
      ]
    },
    {
      "project_name": "Swagathon",
      "likes": 20,
      "submitted_to": "Porticode",
      "winner": false,
      "created_by": "https://devpost.com/AbhinathK",
      "description": "Your laptop stickers are evaluated with OpenCV. You are given a 'hacker score', so you can compare with others.\n\nMost of us have stickers on their laptop. And who does not think they have the coolest. Well, know you can find out if you are right with the use of our app.\n\nUsing OpenCV to calculate a 'hacker score' based on the stickers you have on your laptop cover.\n\nThe project was build with Android Studio and OpenCV.",
      "technologies": [
        "android-studio",
        "opencv"
      ]
    },
    {
      "project_name": "Sumantha",
      "likes": 9,
      "submitted_to": "Porticode",
      "winner": true,
      "created_by": "https://devpost.com/martiserra",
      "description": "Sumantha helps you catch up with your slack channels, showing you the last most important messages.\n\nWe all love Slack, but sometimes in big teams it is impossible to keep up with all the messages.\n\nSumantha works with a Slack command '/catchup' or by contacting her as with other bots. Sumantha is also accessible through a Echo Skill, which makes it really convenient to use.\n\nWe are using a AWS Lambda function that gets POSTS requests from the Bot and answers back with the most important messages. In order to calculate the relevance of the messages, we take into account several parameters:\nNumber of reactions\nLength of the message\nMentions to the user and/or channel\nFrequency of the messages, to know which messages triggered a discussion",
      "technologies": [
        "amazon-alexa",
        "amazon-web-services",
        "lambda",
        "node.js"
      ]
    },
    {
      "project_name": "GBTLC",
      "likes": 6,
      "submitted_to": "Porticode",
      "winner": true,
      "created_by": "",
      "description": "Game Boy Todo List in C\n\nI love Game Boys! I own quite a lot of them, and I've been wanting to write a game for one for quite a while. I have this flash cartridge that lets you run whatever you want on the Game Boy.\nBecause I'm really bad at coming up with game ideas, I ended up making something 'useful' instead!\n\nIt's a todo list that runs on a Game Boy! Because the Game Boy is such a versatile platform, this software is compatible with the original GB, the Color, and all models of the Advance.",
      "technologies": [
        "c",
        "gameboy",
        "gbdk"
      ]
    },
    {
      "project_name": "Aeye.space",
      "likes": 19,
      "submitted_to": "TechCrunch Disrupt London",
      "winner": true,
      "created_by": "https://devpost.com/paulegan",
      "description": "An artificial eye for the visually impaired and blind\n\nMy aunt has an eye disease called Glaucoma which damaged the optic nerves in her eyes. The damage has resulted in severe vision loss, meaning she struggles to find everyday items around her. With advancements in computer vision, it seemed reasonable that we could replace her eyes with an artificial one.\n\nAEye is an artificial eye. Using a combination of the microphone and camera on the device, someone is able to ask where an object is and be guided to it. Moving around the room will result in a \"hot\" or \"cold\" reading.\n\nSplit into two teams: 3 people concentrating on the mobile app & the UX design; 2 concentrating on the backend and visual recognition training.\nWe have trained multiple image classifiers on the Watson visual recognition service. This can give a reliable match when the image contains the object we're searching for. This service is wrapped by a Python web service, which maps the desired object class to the associated classifier. This is hosted on a Radix domain, api.aeye.space.\nThe iOS app uses the Watson Speech to Text service to translate voice input from user. Once the user selects a class to search for, we give aural feedback (\"warmer\", \"colder\", \"found it\", etc) as the user moves the phone's camera around the room or surface.",
      "technologies": [
        "ibm-watson",
        "python",
        "radix",
        "swift"
      ]
    },
    {
      "project_name": "EasyDial",
      "likes": 16,
      "submitted_to": "TechCrunch Disrupt London",
      "winner": true,
      "created_by": "https://devpost.com/jsharm",
      "description": "Hate calling customer support? Alexa will do it for you!\n\nCalling customer support is painful, you have to navigate a menu then wait on hold for 30mins...  When all you want to do is go live your life without your phone to your ear!\n\nWe created an Alexa Skill that calls a company's customer service team on your behalf.  It then chooses the correct menu option, waits on hold until finally you reach a human customer service agent and at that point forwards the call onto you.\n\nAlexa connects to a AWS Lambda function, this makes a request to a Flask server.  This then uses Twillio to create a conference call, adding the company and our AI agent.  The agent then selects the appropriate option for the users query with IBM Watson's Speech to Text and Alchemy Language tools.  When a customer representative answers, the user is then connected!\nThe way we do it with Watson is to take Taxonomy, Sentiment and Keywords, which are then analysed and weighted to determine the best matching response.",
      "technologies": [
        "alexa",
        "aws-lambda",
        "ibm-watson",
        "python",
        "twilio"
      ]
    },
    {
      "project_name": "Cube Runner - Staff Pick",
      "likes": 9,
      "submitted_to": "Local Hack Day III",
      "winner": false,
      "created_by": "https://devpost.com/guso9085",
      "description": "An challenging endless platformer that gets faster the farther you go, how long can you survive?\n\nUsually I do web development, so I wanted to branch out and try something a bit more fun.\n\nIt is a side-scrolling endless platformer. The platforms are randomly generated, and as you go the level starts scrolling faster and faster. To counter your players speed, your player is increasing at the exact same speed as well. I chose to do a simple style, only utilizing flat colors and clean shapes. The first time you contact a platform it changes colors and flashes random different colors as long as your contacting it. Also, you are able to wall jump if you hit the side of platform correctly.",
      "technologies": [
        "pygame",
        "python"
      ]
    },
    {
      "project_name": "PopMidi",
      "likes": 9,
      "submitted_to": "Cipher Presents: Local Hack Day",
      "winner": true,
      "created_by": "https://devpost.com/simonguozirui",
      "description": "This instrument brings a new sense of fun and uniqueness with its cool resonating sound and eco-friendly nature!\n\nWe essentially had three plans: a muchanical music box, a dab machine, and a water bottle flipper.\nSeeing and listening to the ever popular \"floppy disk music\" Imperial March on YouTube inspired us to create our own mechanical music box.\n\nOur hack is able to play music by hitting the cans.\nIt has 8 cans for 8 notes (C, D, E, F, G, A, B, C), and 8 servos to hit the cans.\nIn addition, it is able to take a MIDI file (a form of audio files) and play the notes.\nThat means you can import whatever music you and the machine will play it!\nIt also have a cool dabbing machine which dabs all the time.\n\nThree things needed to be done for the mechanical part of the hack:\nStructure - Building the frame with the cans and the boxes.\nServos - Attaching 9 servos and their respective stick/coin arms.\nTuning - Filling the cans with water to tune them to their appropriate pitch.\nElectrical consisted mainly of wiring 9 servos to the Arduino board and mapping out the contraption.\nWe use Arduino UNO to control servos and 9V batteries.\nThere were two main parts to the programming:\nArduino to Servo - Outputting information the servos to the Arduino, fine tuning the parameters to hit the cans correctly.\nMIDI to Serial - Parsing MIDI files and sending them to the Arduino code as easy to read information in hitting the cans (done in Python).",
      "technologies": [
        "arduino",
        "cardboard",
        "iot",
        "midi.js",
        "pop-cans",
        "python",
        "servo"
      ]
    },
    {
      "project_name": "echran",
      "likes": 4,
      "submitted_to": "Local Hack Day III",
      "winner": false,
      "created_by": "",
      "description": "A proof of concept truly random number generator using echos.",
      "technologies": [
        "rust"
      ]
    },
    {
      "project_name": "visionOS",
      "likes": 14,
      "submitted_to": "Junction 2016",
      "winner": false,
      "created_by": "https://devpost.com/aleks_romanov",
      "description": "An innovational publishing platform for interactive ads and apps\n\nHave you ever missed a bus and have been caught up waiting with no things around except the bus stop? Isn't it boring to have the smartest technologies in the world and still having plain static ads shown on the bus stop billboards? Why the content creators are deprived of the tools and capabilities which mobile app developers have? We believe it is a time to open a new vision, the vision where ad creators become app developers having the opportunity to deliver engaging, IOT enabled and context based ads frictionlessly. We introduce visionOS.\n\nAt its roots visionOS is an operating system running inside the JCDecaux bus stop. It provides the essential API services to let ad creators access data from various sensors like camera, motion tracking and thermometers. In addition, visionOS makes it possible to include third party API's into the app like Deezer, Snapchat or Swarm.\nAll uploaded ads to the JCDecaux content server are then uploaded to the bus stations running visionOS. Ads have metadata configured before it is uploaded to the JCDecaux CMS. The data defines to which stations it must be sent (Finland, Sweden, France, Germany … maybe all. It is possible to configure targeting on the per-station basis). Vision OS runs the ads one by one and checks two conditions (questions).\nShould the ad be shown at this moment of time?\nWhat is the money proposal for being shown?\nThe add to be shown is analysing the environment by using visionOS sensors API. The criteria includes:\nLocation and recent news (near a conference?) -----> show a car ad\nWeather ---> raining ? -----> show umbrellas\nBuses are late? ----->  Show a car ad if people are at the stop\nAnd any other criterias, the advertiser wants to consider. Apps have full access to the Internet and can evaluate any condition to get their value proposition right.\nThe content and the way how IOT and third party API's are used are up to developer what generates a great space for creativity.\n\nWe built visionOS with the help of Unity technologies and prototyping tools like Sketch. Along with the project we created the sample use cases for engaging ad campaign.",
      "technologies": [
        "c#",
        "creativeminds",
        "jcdecaux",
        "keynote",
        "sketch",
        "unity"
      ]
    },
    {
      "project_name": "Hospital Adventure",
      "likes": 39,
      "submitted_to": "Junction 2016",
      "winner": true,
      "created_by": "https://devpost.com/RobertStark",
      "description": "Reinvent a child's and parents' hospital experience by making the whole process transparent and fun!\n\nA child's hospital experience is often frightening, exhausting and overall unpleasant. We change that by creating a unique storyline for a child's hospital stay.\nUsing the GE Adventure Series as inspiration, we wanted to imbed the awesome settings into a comprehensive use case.\n\nOur application allows us to portray a child's hospital stay in a unique story, while simultaneously informing the parents about all necessities and the status of the treatment.\nParents and children are able to login into different views, using the child's unique patient ID.\nChild's view: The child receives a virtual guide through his journey, mapping the upcoming treatment steps to a fun storyline. By integrating the hospital's environment and equipment, the child looses its fear of upcoming procedures. An avatar accompanies the child on his journey and explains every step in a simple and visual manner.\nParents' view: By including the GE Opera API, parents receive live updates about the treatment progress and scheduled examinations.\nFurthermore, parents have a dashboard, allowing them to support the child during the whole affair, as well as being prepared for all eventualities.\nIn stress situations, things tend to get overlooked. By providing extensive checklists, the parents are able to focus on more important aspects.\n\nUsing react-native, we simultaneously built one app for IOS and Android devices. It connects to our simulated GE Opera API, allowing us to display realtime status updates.",
      "technologies": [
        "ge",
        "javascript",
        "lots-of-love",
        "python",
        "react-native",
        "redux",
        "ruby-on-rails"
      ]
    },
    {
      "project_name": "Room service",
      "likes": 9,
      "submitted_to": "HackKing’s 3.0",
      "winner": true,
      "created_by": "https://devpost.com/Ifor",
      "description": "Check in a hotel or odder room service using Amazon Echo and your mobile phone\n\nSami and hotels, and Mihai and hotels, and Whitbread challenge.\n\nYou can check in hotel without interacting with human being. You can do everything by speaking with Amazon Echo. Also you do not have to call or go downstairs to order room service. You can do it using text messages.\n\nWe used Amazon Echo and AWS lambda to host Alexa skill. We also used Twilio API to handle text messages.",
      "technologies": [
        "alexa",
        "amazon-web-services",
        "claudia.js",
        "echo",
        "github",
        "lambda",
        "node.js",
        "twilio"
      ]
    },
    {
      "project_name": "eyeServant",
      "likes": 13,
      "submitted_to": "Junction 2016",
      "winner": true,
      "created_by": "https://devpost.com/PawelKupsc",
      "description": "Get your stuff in the blink of an eye! The eyeServant will pick up and deliver whatever you look at.\n\nWe were inspired by the eternal human laziness. Nothing to be ashamed of, we just say it as it is. Humans go the great lengths to save themselves some unwanted effort – and we aim to help them to do so.\nBut jokes aside, the main purpose of the solution is to help people with limited mobility in their everyday lives in a truly simple and unintimidating way.\n\nThe goal was to control the Baxter robot with the Tobii EyeX eyetracker and make it move objects from one place to another. The user would look at the object and then look at the destination where the object should be placed – and consequently the robot would do all the work for the user and move the object.\n\nInstead we pre-programmed sequence of movements via the robot interface. Then we created the Unity program that reads the data from the eyetracker and checks what is the user looking at. If the user looks at one of the marked spots (marked specially in the Unity app for the purpose of the presentation), the signal is sent from the computer, via the Arduino board, to the Baxter robot, and as a result Baxter performs the next movement from the sequence.",
      "technologies": [
        "arduino-uno",
        "baxter-robot",
        "tobii-eyex",
        "unity"
      ]
    },
    {
      "project_name": "amp",
      "likes": 9,
      "submitted_to": "GreatUniHack Fall 2016",
      "winner": false,
      "created_by": "",
      "description": "Wireless amp system using mobile phones and WebRTC\n\nWireless amp systems are really cool and convenient, but they're also crazy expensive. I thought I'd really like one but there was no way I could afford a professional one, so what other option did I have but to make my own?! After recently seeing a talk about the Web Audio API at a conference, I really wanted to try it out, so I thought I would!\n\nUsing an iRig (or some other device/cable) you plug your guitar into your phone and open up the 'play' page of amp in a browser. Then open the 'listen' page on another device and you'll be able to hear the amplified guitar playing - no wires between the two!",
      "technologies": [
        "guitar",
        "javascript",
        "node.js",
        "web-audio-api",
        "webrtc"
      ]
    },
    {
      "project_name": "SMT Reflow Oven",
      "likes": 6,
      "submitted_to": "Cal Hacks 3.0",
      "winner": false,
      "created_by": "https://devpost.com/bryancole",
      "description": "SMT soldering is a pain to do by hand. Reflow ovens are nice, but expensive. We built a toaster reflow oven.\n\nA reflow oven is used to solder multiple surface mount circuit board components at one time. The surface mount components are all arranged appropriately on the board with a liquid solder paste adhering them to the board. The whole board is then placed in the reflow oven and put through a heat profile set up for the specific type of solder paste used. When the oven reaches its terminal temperature the solder paste melts, then once cooled it solidifies, creating a solid weld between the component and the board.\n\nWe bought a toaster on Craigslist for $20 a couple of hours before CalHacks. This oven was then disassembled, removing all of the analog controls, leaving us with a metal box with four quartz heating elements rated to draw up to 1300 watts. The elements being quartz are important part of a good home reflow oven. They create a more even heat and don't heat up black components nearly as much as infrared heating elements. We then controlled the heating elements with two solid state relays. This allowed us to turn on and off the heating elements quickly to control the temperature. The temperature sensing was done with a k-type thermocouple which can read temperatures up to 1400 degrees Celsius. While our oven will never get that hot, it is nice to know that we won't hit an upper limit on temperature when reflowing which could happen with other types of temperature sensors.",
      "technologies": [
        "arduino",
        "c++",
        "esp8266"
      ]
    },
    {
      "project_name": "Spot",
      "likes": 8,
      "submitted_to": "YHack 2016",
      "winner": false,
      "created_by": "https://devpost.com/PennRobotics",
      "description": "A personal companion that acts like a dog or dances to music, developed using iRobot's Roomba platform\n\nWe are a group of students passionate about automation and pet companions. However, it is not always feasible to own a live animal as a busy engineer. The benefits of personal companionship are plentiful, including decreased blood pressure. Automation is the way of the future. We developed routines using a iRobot Create 2 robot which can dance to music, follow its owner like a dog, and bring items from another room on its top.\n\nSpot uses visual processing and image recognition to follow its owner all over their home. He is a helpful companion capable of carrying packages, providing lighting and cleaning for his owner. Furthermore, his warm and friendly appearance is always welcome in any home. The robot platform used also has the capability for autonomous floor cleaning. Finally, Spot's movements can be controlled through a web application which also displays graphs from all the Roomba's sensors.\n\nSpot was built using a variety of different software. The webpage used to control spot was coded in HTML and CSS with Django/Python running in the backend. To control the roomba and display the sensor graphs we used matlab. To do the image processing and get the roomba to follow specific colours the openCV library with python bindings was used.",
      "technologies": [
        "css3",
        "django",
        "html",
        "love",
        "matlab",
        "python"
      ]
    },
    {
      "project_name": "Beyondagon",
      "likes": 10,
      "submitted_to": "MLH Prime Southwest Regional 2016",
      "winner": false,
      "created_by": "https://devpost.com/tommy__123__",
      "description": "A format for storing models of infinite detail in a tiny file.\n\nOur inspiration was to take the svg format for 2d graphics and make a version for 3d that kept all the benefits!\n\nOur project defines a format for 3d models that allows us to have an \"infinite\" resolution while keeping a file size that is incredibly small.\n\nWe coded the project purely in Swift 3.0.",
      "technologies": [
        "swift"
      ]
    },
    {
      "project_name": "Autonomous Airplane Pushback Vehicle",
      "likes": 11,
      "submitted_to": "Aerohacks",
      "winner": false,
      "created_by": "https://devpost.com/boalin",
      "description": "This vehicle is a semi-autonomous airplane pushback vehicle. This only a watcher is required for airplane taxis.",
      "technologies": [
        "arduino",
        "bluetooth",
        "buzzer",
        "c++",
        "color-leds",
        "crash-detector",
        "interrupt-routine",
        "ios",
        "light-sensor",
        "mbed",
        "mcookie",
        "motor",
        "oled",
        "sensors"
      ]
    },
    {
      "project_name": "Pharmassist",
      "likes": 18,
      "submitted_to": "health++",
      "winner": true,
      "created_by": "https://devpost.com/heidihuang",
      "description": "Accessible Prescription Labels\n\nPrescription labels are not accessible to everyone. Confusing the dosage, frequency, and timing of prescription drugs can result in unnecessary expenditure of resources, increased morbidity, and ultimately patient fatality. Current solutions are too expensive, bulky, or cumbersome to use.\nThere are more than 285 million visually impaired people around the world. While the idea was motivated by concern for blind and visually impaired patients, it has high potential for application to wider audiences. According to a study performed at University of College London, a third of the elderly population (age 65 and older) is in danger of premature death due to misunderstanding medicine labels. A study of low-income patients carried out at Northwestern University showed that nearly half of even visually able participants misinterpreted at least 1 out of 5 prescriptions labels presented. With the disparate education and literacy levels across the world, Pharmassist has the potential to improve the lives of billions of people.\n\nPharmassist takes advantage of using low-cost conductive ink to print distinct patterns onto labels, which can be placed directly onto pill bottles. These patterns simulate multi-touch inputs and, in combination with our smartphone application, can be recognized to identify and access a patient’s specific prescription information, which will be presented audibly. Additional capabilities include being able to track a patient’s usage and offer on-demand services, such as options to refill prescriptions, request side effect information, and directly contact their pharmacist.\n\nThe Pharmassist smartphone application was built using Java and Android Studio. We used conductive copper tape in place of conductive ink in the development of our prototypes. By using the tape to create different patterns on the bottoms of pill bottles, we were able to simulate recognizable touch patterns that could be identified by our application and used to relay specific prescription information.",
      "technologies": [
        "android-studio",
        "java"
      ]
    },
    {
      "project_name": "Amviewlate",
      "likes": 8,
      "submitted_to": "ArchHacks",
      "winner": false,
      "created_by": "https://devpost.com/pierreamelot",
      "description": "AR illusions to enhance mobility in people with Parkinson's disease\n\nThrough our research, we discovered that many of the limitations experienced by people with Parkinson’s disease can be overcome with perceptual tricks. While the pace of walking on a flat surface is staggered and interrupted, the continuous motion of using a staircase--or even the illusion of a staircase--can enable a person with Parkinson’s to walk with a regular gait. And while many encounter difficulty extending an arm to interact with an object, the introduction of a moving target provides an active stimulus that prompts a response.\nClick here for the inspiration behind our project (3:45 - 5:05).\nWe thought: What if we take this painted staircase illusion and apply it anywhere in the world?\n\nAmviewlate (a riff on \"ambulate\" and \"view\") assists with two key problems:\nFreezing of gait, an ambulatory disturbance that results in slow, shuffled walking\nDyskinesia, the impairment of voluntary movement, particularly without stimuli to trigger reflexes\nAmviewlate projects visual cues in augmented reality to facilitate walking and interacting with objects. A staircase illusion helps simulate the act of climbing stairs, enabling continuous motion. A visual cue guides the hand to an intended target. All actions are governed by voice control software to enable full agency in users with limited gesture control. Through Amviewlate, we hope to restore mobility, independence, and confidence in our users.\n\nFor this project, we quickly set up an AR application using Google Cardboard and a marker recognizer library called Vuforia. The next challenge was to integrate speech recognition as part of the app. There were no free Android speech recognizer assets available on Unity, so we had to build one ourselves by integrating Android plug-ins to the Unity project. Finally, we set up the whole scene in the application to provide a compelling experience for people with Parkinson's disease.",
      "technologies": [
        "android-studio",
        "blender",
        "google-cardboard",
        "unity",
        "vuforia"
      ]
    },
    {
      "project_name": "Wind Power",
      "likes": 9,
      "submitted_to": "Hoya Hacks",
      "winner": true,
      "created_by": "",
      "description": "A low-cost alternative to complex overpriced wind tunnels\n\nThe idea was to make a low cost wind tunnel that can be used in a wide variety of applications. This is especially beneficial to educational institutions who do not necessarily need a research level wind tunnel.\n\nThis will measure wind speed, temperature, and humidity of the air in addition to allowing you to view the air around the object in question.\n\nWe gathered our strong minds and found resources from various everyday objects around Georgetown. We constructed the actual tunnel from goldfish boxes. We used a motor and propeller to simulate wind. Cups with a magnet are used to detect wind speed.",
      "technologies": [
        "arduino",
        "goldfish",
        "heart",
        "love",
        "node.js",
        "raspberry-pi",
        "soul",
        "wire"
      ]
    },
    {
      "project_name": "Selfie Stick Golf",
      "likes": 8,
      "submitted_to": "HackNC",
      "winner": true,
      "created_by": "https://devpost.com/hgreer",
      "description": "Put your phone in a selfie stick and it becomes a golf club!\n\nSelfie stick looks like a golf club\n\nconnects your phone to your computer so you can use it as a controller for a golf video game",
      "technologies": [
        "javascript",
        "phonegap",
        "python"
      ]
    },
    {
      "project_name": "The Grand Tour",
      "likes": 5,
      "submitted_to": "HackNC",
      "winner": false,
      "created_by": "https://devpost.com/benl12",
      "description": "Using our innovative software and the Esri API fly around and see the world like never before!\n\nThe inspiration from this project came from our natural curiosity and desire to travel and see the world. However, being college students this would be prohibitively expensive. So instead, we wanted to do the next best thing. What if you could use an immersive map and controller to fly around and visit these places? By just typing in a location, you would be able to visit anywhere in the world and immerse yourself in the scenery. This simple concept formed the basis for our application.\n\nUsing this application, you are able to take a \"grand tour\" of the world. Set up your tour route and then launch the application and watch as you are taken to each of these locations. At each location, you are able to use the Xbox controller to fly around and explore, spending as much time as you want before moving onto the next location on your tour. Additionally, we pseudorandomly generate checkpoints on the map that you must find before moving to the next tour location. This element allows the application to also function as a game - one must explore the world learning about the culture and scenery while trying to find the checkpoint.",
      "technologies": [
        "esri",
        "java",
        "xbox"
      ]
    },
    {
      "project_name": "Kine-Fanatic",
      "likes": 7,
      "submitted_to": "CUES Hackathon 2016 Powered by ARM",
      "winner": false,
      "created_by": "https://devpost.com/dabus18",
      "description": "Kinesthetic learning meets machine learning\n\nLearn how to effectively throw a ball.\n\nThe Kine-Fanatic allows us to monitor the accelerations of different critical points on the body in movements like throwing, punching, or shooting a basketball. After analyzing multiple actions, we could classify newly recorded actions based on the data we collected.\n\nWe accomplished data collection and transmission with BBC Micro:bit programmed in C++. Three sensor micro:bits sent acceleration data to the main hub micro:bit which had a serial link to a computer. We then imported the data into Matlab for analysis and classification.",
      "technologies": [
        "c++",
        "matlab"
      ]
    },
    {
      "project_name": "Hack the Accessibility",
      "likes": 5,
      "submitted_to": "BrumHack 5.0",
      "winner": true,
      "created_by": "https://devpost.com/razvangeangu",
      "description": "Alexa for mute people: check your account balance, make payments and invest using Sign Language!\n\nWe were looking for inspiration and someone said \"do something a CEO can use\". We decided to create a fintech app, using CapitalOne and BlackRock APIs to generate the best three options to invest, looking at different ratios and returns (peRatio, pbRatio, returnonEquity, returns from year 1 and year 5). Furthermore, we wanted to make it more useful, so, we had the idea to implement this project for projects with disabilities.\n\nUsing American Sign Language, you can ask Alexa to do different tasks for you: get your account balance, make payments for you, generate the first three best investment options and interact with Alexa during executing those tasks.\n\nLeap-Motion: we created a machine learning program, which \"reads\" the signs and generate a string, which is sent to Alexa\nCapitalOne: we used their API to access a set of accounts, in order to make a payments and to generate the balance for the user's account \nAlladin: we used their API to access different investment portfolio and through an econometrics model, it returns your best three options for investment \nAlexa: we integrate everything as an Alexa's skill",
      "technologies": [
        "alexa",
        "alladin",
        "javascript",
        "leap-motion",
        "nessi",
        "node.js"
      ]
    },
    {
      "project_name": "Project Firefly",
      "likes": 16,
      "submitted_to": "HackRU Fall 2016",
      "winner": true,
      "created_by": "https://devpost.com/timotius",
      "description": "A new form of light painting\n\nLight Painting is an photographic technique which is made by taking a long exposure photograph on moving handheld sources of light. There are several problems with this approach:\nThere is no feedback system. The artist is unable to see exactly what they're drawing\nThere is a limited amount of time to take a long exposure shot, and on top of that there are no room for mistakes\nIt's a difficult technique; creating complex art requires practice and skill\nAll artwork made with conventional light painting techniques seem hand-drawn. It looks like someone just drew their artwork with a marker\n\nSo in traditional light painting the artist just sets the camera to long exposure and waves a flashlight around at it to create their artwork. So, what if you had 288 LEDs on a two meter long stick? One can upload an image to this stick, then hold it upright, and as they move it through 3D space, the stick scans through the image and shoots out each column of pixels in light. It's essentially a 3D light printer.\n\nWe used a Raspberry Pi 3, a two-meter-long strip of 288-LED tape we soldered together, a breadboard, and various electrical components for our circuits. The Raspberry Pi intakes images through a USB drive and processes them with code in Python and C.",
      "technologies": [
        "camera",
        "led",
        "python",
        "raspberry-pi"
      ]
    },
    {
      "project_name": "QuicKicks",
      "likes": 4,
      "submitted_to": "HackTX 2016",
      "winner": false,
      "created_by": "https://devpost.com/chrisayoub",
      "description": "Guarantees a pair of the hottest new sneakers.",
      "technologies": [
        "android-studio",
        "htmlunit",
        "http",
        "java",
        "javascript"
      ]
    },
    {
      "project_name": "Diffy",
      "likes": 6,
      "submitted_to": "HackHarvard 2016",
      "winner": false,
      "created_by": "https://devpost.com/JacobMiske",
      "description": "Diffy sends alerts about what has visually changed over periods of time\n\nOne of our team members regularly purchases canned soup, and she ran out without realizing it. She wondered if she could create something to automatically order more food when she ran out.\n\nDiffy uses computer vision and OpenCV and RoboRealm to identify visual changes, to alert users when something is no longer there. \nOur demo involves identifying when things are missing from a pantry, so that new food can be automatically ordered.\n\nWe first constructed a demo shelf, stocked with standard food items, and set up a live webcam feed. Then, we used OpenCV and RoboRealm to separate the output of the webcam into distinct frames and analyze sequential frames for visual changes, correcting for minute changes that we don't care about as well as changes in lighting. We then used Microsoft's Cognitive Services API for image recognition, the result of which is sent to a text-to-speech package to be read aloud.  Finally, we built an Arduino with an ultrasonic distance sensor, so that the camera's output is only used and analyzed when motion is detected (the Arduino also helps the system avoid detecting a change when a person is between the camera and the shelf.",
      "technologies": [
        "arduino",
        "microsoft-office-research",
        "microsoft-project-oxford",
        "opencv",
        "python",
        "roborealm"
      ]
    },
    {
      "project_name": "Brakedown",
      "likes": 6,
      "submitted_to": "HopHacks - Fall 2016",
      "winner": false,
      "created_by": "https://devpost.com/jhbae",
      "description": "Brakedown is a \"collaborative\" driving web game, where everyone controls one car through an obstacle course.\n\nThis is an .io type massively multiplayer online game with a twist - Twitch Plays!\n\nAll of the players control a single car; the degree of steering is determined by the average of everyone's input. Players are randomly distributed to 2 teams, good and evil. The good players win if the car safely reaches the end of the track, while the evil players win if the car crashes into an obstacle or drives out of bounds.\n\nWe used Node.js and Express for the back-end framework and used HTML Canvas and JavaScript for the user interface and animations. We used socket.io for the client-server communications.",
      "technologies": [
        "express.js",
        "html5",
        "javascript",
        "node.js",
        "socket.io"
      ]
    },
    {
      "project_name": "Distill",
      "likes": 7,
      "submitted_to": "DubHacks 2016",
      "winner": false,
      "created_by": "https://devpost.com/penandlim",
      "description": "A Chrome extension that uses machine-learning to block images and text that may be harmful to the end user.\n\nWe wanted to enable safe browsing for people for whom browsing the web is inherently risky. This includes everyone from recovering drug addicts, to PTSD victims, to children.\n\nDistill is a Chrome extension that censors photos and words based on the machine-learning API Clarifai and different genres of censorship.\n\nWe used JavaScript, Clarifai API, CSS, and HTML. Using such a wide variety of programming skills meant we had to work as a team to balance each other's strengths and weaknesses.",
      "technologies": [
        "clarifai",
        "css",
        "html",
        "javascript"
      ]
    },
    {
      "project_name": "Tetr.js",
      "likes": 8,
      "submitted_to": "Hack Western 3",
      "winner": false,
      "created_by": "https://devpost.com/willclark",
      "description": "Solving Tetris with a genetic algorithm.\n\nI don't like playing Tetris. The game is dull, monotonous, and ultimately self defeating. And to add insult to injury, I'm not even particularly good at it.\nSo, like any good aspiring engineer, I decided to take the lazy approach, and built a JavaScript-based AI to play it for me. Tetr.js uses a machine learning algorithm to continuously refine the weightings of the parameters which define a good move.",
      "technologies": [
        "css",
        "html",
        "javascript",
        "jquery"
      ]
    },
    {
      "project_name": "Braille Printer",
      "likes": 22,
      "submitted_to": "Hack Western 3",
      "winner": true,
      "created_by": "https://devpost.com/pwnedpixel",
      "description": "A quick, affordable method to print braille.\n\nSmall scale braille printers cost between $1800 and $5000.  We think that this is too much money to spend for simple communication and it has acted as a barrier for many blind people for a long time.  We plan to change this by offering a quick, affordable, precise solution to this problem.\n\nThis machine will allow you to type a string (word) on a keyboard.  The raspberry pi then identifies what was entered and then controls the solenoids and servo to pierce the paper. The solenoids do the \"printing\" while the servo moves the paper.\nA close-up video of the solenoids running: https://www.youtube.com/watch?v=-jSG96Br3b4\n\nUsing a raspberry pi B+, we created a script in python that would recognize all keyboard characters (inputted as a string) and output the corresponding Braille code. The raspberry pi is connected to 4 circuits with transistors, diodes and solenoids/servo motor.  These circuits control the how the paper is punctured (printed) and moved.\nThe hardware we used was: 4x 1n4004 diodes, 3 ROB-11015 solenoids, 4 TIP102 transistors, a Raspberry Pi B+, Solarbotic's GM4 servo motor, its wheel attachment, a cork board, and a bunch of Lego.",
      "technologies": [
        "blood",
        "hardware",
        "python",
        "raspberry-pi",
        "sweat",
        "tears"
      ]
    },
    {
      "project_name": "Habbit",
      "likes": 8,
      "submitted_to": "Hack Western 3",
      "winner": false,
      "created_by": "https://devpost.com/1vn",
      "description": "slowly kill your reddit addiction\n\nWe were all chronically addicted to Reddit, and all the productivity extensions out there took a \"cold turkey\" approach. We felt like our method of gradual addiction treatment is more effective.\n\nOver time, we slowly remove elements on reddit that are addicting (CTA's, voting counters, comment counts, etc.). This way, the user willingly develops indifference towards the platform opposed to fighting the hard wired behaviour that Reddit expertly ingrained.\n\nWe performed research on how drug rehabilitation is performed at rehab centres, and incorporated those practices into the design of the extension. We used jQuery to manipulate the DOM elements.",
      "technologies": [
        "chrome-extension-api",
        "javascript",
        "jquery"
      ]
    },
    {
      "project_name": "Fizz Filter",
      "likes": 12,
      "submitted_to": "Reality, Virtually, Hackathon",
      "winner": false,
      "created_by": "https://devpost.com/yannick_boers",
      "description": "Choose Your Flavor\n\nSnapchat uses augmented reality to dynamically apply lenses to users' faces. What if you could apply a lens similarly to alter the environment around you? We thought it would be interesting to instantly be able to redesign any room you're in to transport you into a completely different location. What if you wanted to feel the warmth of those nights in the ski lodge, next to the fireplace and looking out of the window and watching as the snow falls down. Or what if you were alone that night, but wanted to be around people, while all of your friends were busy at home? You can redecorate, or reimagine, your current location with Fizz Filter, which brings a spark to your life at any time you want.\n\nFizz Filter is an augmented reality application tailored for ease of use and scalability of filters. You enter a simple environment where you are given a choice of how you want your environment to be reimagined. Then, your environment is scanned by the Hololens and soon enough your room is populated with decorations and other indicators that has you feeling the theme that you chose.\n\nWe utilized the HoloLens' spatial mapping and spatial understanding capabilities to recognize the features of room the user is in. Then, we used a smart placement algorithm to determine how to distribute the objects across the space based on various input factors. About 15% of our project was built using Microsoft's HoloLens API.",
      "technologies": [
        "blender",
        "c#",
        "microsoft-hololens",
        "unity"
      ]
    },
    {
      "project_name": "ImaTake",
      "likes": 10,
      "submitted_to": "HackUPC Fall 2016",
      "winner": true,
      "created_by": "https://devpost.com/HEIMDAL13",
      "description": "Take the best images automaticly, in seconds.\n\nWhen you return from a trip, you usually have a lot of repeated photos. There are lots of blurry photos, noise ones, and even some of them have awful exposition. Selecting the best ones is a very time consuming task, so we wanted to solve that issue.\n\nIt's a computer software that detects similar images from a input set and it selects the best ones automatically.",
      "technologies": [
        "matlab"
      ]
    },
    {
      "project_name": "Jedi Maze Game",
      "likes": 8,
      "submitted_to": "HackUMass IV",
      "winner": false,
      "created_by": "https://devpost.com/AlexDFischer",
      "description": "Move a ball through a maze without touching either, via Leap Motion.\n\nYou know those games where you move a ball through a maze? What if we made it even harder?\n\nBy using a Leap Motion, simply rotating your hand rotates a maze. You can move a ball through a maze even if you're a thousand miles away!\n\nWe 3d printed the maze and other parts that control rotation. A Raspberry Pi controls 2 servo motors. The 2 servos control the pitch and roll of the maze.",
      "technologies": [
        "3dprinting",
        "leap-motion",
        "raspberry-pi",
        "servo"
      ]
    },
    {
      "project_name": "Stroke of Genius",
      "likes": 4,
      "submitted_to": "MHacks 8",
      "winner": false,
      "created_by": "https://devpost.com/hxu28602",
      "description": "A personal ping pong trainer in the palm of your hand!\n\nWe know that traditional sports analytics are pretty triggering-- they’re reactive, not corrective. Mere analysis of improvement after a workout is nowhere near as effective as a device that measures and actively improves your stroke in realtime, so we made one.\n\nThe Stroke of Genius provides both analytics and realtime correctional assistance. With cameras and sensors embedded into the paddle, data mapping the speed, orientation, and position of a user’s wrists are processed in a constant stream. Motors on the edges of the paddle vibrate in reaction to incorrect form, modifying angle of swing and gently correcting the user’s orientation. Within the IMU, a gyroscope and accelerometer track stability and fluidity of movement, lightly vibrating the motors when jerkiness passes a certain threshold.\n\nWe designed and 3D printed a custom paddle design with ridges for embedded sensors and a hollow handle for any wires. With the Teensy Arduino, we embedded six vibrational motors on the sides and top of the paddle and mounted a PS3-eye camera to the back. We also used an IMD for its gyroscope and accelerometer.",
      "technologies": [
        "c++",
        "imu",
        "makerbot",
        "ps3-eye",
        "python",
        "svo"
      ]
    },
    {
      "project_name": "SurfShield",
      "likes": 7,
      "submitted_to": "HackDartmouth III",
      "winner": true,
      "created_by": "https://devpost.com/AndrewOgren",
      "description": "SurfShield is an extension for Chrome, and it's purpose is to warn users about cyberbullying and offensive content.\n\nOur inspiration for SurfShield was the Hack Harassment initiative. The internet can be very useful, but due to its anonymity, it can also be used for cyberbullying and for posting vulgar and offensive content. We wanted to develop a way to let online users know whether the content of the page they are visiting is considered offensive. To do this effectively, we incorporated various apis and aggregated metrics that help to better inform the user.\n\nSurfShield is a chrome extension. When a user visits a site, the SurfShield icon will change colors to display the level of offensive content on the page with green representing little to no offensive content and red representing a high level of offensive content. When the user clicks on the SurfShield icon, the extension will display an overall score, which is between 1 and 5. This score is averaged from from our four main metrics, which are anger, cyberbullying, profanity, and if users have voted on that particular site, an audience score.\n\nWe built SurfShield by creating a Chrome Extension using javascript, html5, and css3. The API calls are made in Python. Using Amazon Web Services, we process the results and then update the UI of the chrome extension with the anger, cyberbullying, profanity, audience score, and overall score. The SurfShield icon also changes colors based on the overall score.",
      "technologies": [
        "amazon-ec2",
        "amazon-rds-relational-database-service",
        "amazon-route-53",
        "amazon-web-services",
        "aws-elastic-beanstalk",
        "bark.us-api",
        "css3",
        "flask",
        "html5",
        "javascript",
        "pip",
        "python",
        "virtualenv",
        "watson-tone-analyzer"
      ]
    },
    {
      "project_name": "IntelliKnee",
      "likes": 5,
      "submitted_to": "HackGT 2016",
      "winner": false,
      "created_by": "https://devpost.com/DavidGoldman",
      "description": "Automatically locking knee brace - when you need it\n\nPeople in physical therapy often need to wear knee braces to help stabilize their knee and add support. However, some knee braces either don't offer enough support or are too rigid for normal use.\n\nOur adaptive knee brace automatically detects dangerous changes in knee position i.e. when your knee locks up and engages more support.\n\nThe main driver of the brace is an Arduino, which uses a rotary sensor to detect dangerous changes in knee position along with a servo to help manage the locking mechanism. The structural parts of the prototype are 3D printed as they're lightweight and still can offer some support.",
      "technologies": [
        "3d-print",
        "arduino"
      ]
    },
    {
      "project_name": "Trojan - Automated Diabetic Retinopathy Detection",
      "likes": 20,
      "submitted_to": "Predix India Hackathon",
      "winner": true,
      "created_by": "https://devpost.com/sadhanag",
      "description": "Ultimate health care tool for predicting visison loss\n\nDiabetic retinopathy (DR) is a disease with an increasing prevalence and the main cause of blindness among working-age population. The risk of severe vision loss can be significantly reduced by timely diagnosis and treatment.\nThe current process of testing for diabetic retinopathy is laborious and often inefficient and the scope of detecting the diseases in the early stages are completely unexploited. There exists a need for a powerful Automated retinal image analysis for early stage Diabetic retinopathy detection\n\nThe tool analyzes the retinal image of a patient to detect whether the patient is affected with Diabetic Retinopathy and if yes, how severe (on a medical scale of 1 to 4; 4 being highest). It is the ultimate tool to aid in precisely detecting the early stages of diabetic retinopathy. .",
      "technologies": [
        "angular.js",
        "cloud-foundry",
        "css",
        "flask",
        "grunt.js",
        "html",
        "javascript",
        "jquery",
        "json",
        "machine-learning",
        "node.js",
        "postgresql",
        "predix",
        "python"
      ]
    },
    {
      "project_name": "Bob's Ramen",
      "likes": 27,
      "submitted_to": "HackCMU 2016",
      "winner": true,
      "created_by": "https://devpost.com/kibowman",
      "description": "Have you ever been starving after classes? Bob's Ramen can remotely pre-prepare ramen with a tap of a phone.\n\nSometimes in college, you don't have time to cook, and you want food ready for you when you get home.\n\nBOB'S Ramen is a combination of an iPhone application and and automatic ramen cooker. Through the app, a user can specify a specific time for ramen to be cooked and a spice level and the ramen cooker prepares it completely autonomously.\n\nThe iOS application was built using swift in Xcode, and the ramen cooker was built using the NodeMCU wireless micro controller, Arduino IDE, servo motors, cardboard, duct tape, innovation, grit, and twelve cups of coffee.",
      "technologies": [
        "arduino",
        "cardboard",
        "duct-tape",
        "innovation",
        "msg",
        "swift"
      ]
    },
    {
      "project_name": "Project Open Heart",
      "likes": 14,
      "submitted_to": "Hack the North 2016",
      "winner": true,
      "created_by": "https://devpost.com/Scub3d",
      "description": "VR Open Heart Surgery With Alexa as a Virtual Nurse\n\nDoctors, Students, and upcoming surgeons are striving to better themselves in private practice or for general surgical tasks. Wether for academic or industrial reasons, upcoming surgeons have barely any information to look at when learning how to do a specific surgery. We made Project Open Heart to provide an opportunity to develop a better understanding of what techniques an methods to use in a surgery and ultimately lock in the concepts/surgical steps through VR.\n\nWe provide a full system to a legitimate surgery. The first part of the system is our \"Nurse,\" Alexa. Alexa helps us retrieve specific tools necessary for our surgery. Through simple requests such as, \"bring me the scalpel,\" Alexa responds by adding a scalpel to your hand in the virtual reality atmosphere. The second part is connecting with the oculus and actually being hands-on with the entire surgery experience. We provide recordings of the surgery so other people can analyze mistakes and show others how the specific surgery is conducted. There is also the feature to record your actions and save them to a file. This file can then be played back at a later date for you or others to see how you performed and what you did. Lastly, you can stream your surgery to any others who want to watch it with our setup. Using firebase, we can sync data between the host and the clients with minimal effort.\n\nThe system has two parts: an Alexa app and a Unity app. The Alexa app leverages the firebase api, amazon lambda, and the Alexa API. Once a certain task is called, we call a function in our Alexa app to send an update to our Firebase Database. This allows for live communication between both the Nurse and Surgeon. Now moving over to the surgeon-side. We use Oculus Rift to allow a full hands on experience of an open heart surgery we used 3d models to show a human body: including skin, flesh, bone, and organs. We attached a Leap Motion to the oculus to allow us to track the hand movements when performing a certain gesture in the surgery. When eventually synced with Firebase, we allow for more accessibility of the surgery performed and that is also done through the Unity application.",
      "technologies": [
        "amazon-alexa",
        "c#",
        "javascript",
        "node.js",
        "unity"
      ]
    },
    {
      "project_name": "UkeMaster",
      "likes": 7,
      "submitted_to": "Hack the North 2016",
      "winner": false,
      "created_by": "https://devpost.com/JeffreyBarton",
      "description": "Self learning a string instrument is difficult. Our device teaches intuitively by guiding students using lasers.\n\nWe have all met many people who want to learn an instrument, guitar and ukulele being the most popular. New learners struggle to learn the basics, such as finger placement, chord progression, and timing. Paying an instructor is very useful but expensive. Today in age, self learning is the way most young people prefer to learn. The internet can give information, but not the intuition needed to really play an instrument.\n\nThe UkeMaster uses an array of low power lasers to show the student where to place their fingers on the ukulele. The app communicates with the UkeMaster to allow the student to select chords they want to learn. Or even a song, where the UkeMaster will show the student how to progress through the chords.\n\nFor the hardware, we used a C.H.I.P microprocessor to control a custom 3d printed array of lasers. The C.H.I.P controls the lasers using a python script we wrote. The python script incorporated pyrebase (a Firebase python wrapper) to communicate via wifi to a database to retrieve the chords to display. \nWe also built an iOS app using swift and Firebase to control which chords to display and when to display them.",
      "technologies": [
        "3dprinting",
        "c.h.i.p.",
        "firebase",
        "hardware",
        "ios",
        "lasers",
        "python",
        "solder",
        "swift",
        "ukulele"
      ]
    },
    {
      "project_name": "Talk Lock",
      "likes": 5,
      "submitted_to": "HackISU",
      "winner": false,
      "created_by": "https://devpost.com/thomashan",
      "description": "Do away with those annoying keys and unlock anything with Talk Lock!\n\nWe wanted to find a way to incorporate the Amazon Echo more seamlessly into everyday life. Then we thought of how outdated the physical key system is and decided to have our own try at modernizing it.\n\nWith Talk Lock, you are able to simulate unlocking a door by giving a passcode. As a user, you are able to preset one of two modes - password mode, which simply prompts you for the password, or the fun and flirty lyric mode, which tosses a random song at you and expects you to complete the missing lyric. With password mode, it is also possible to change the current password after confirming the old password.\n\nWe focused our design around the Amazon Echo. To get our skill up and running, we made a Lambda function to handle the skill-specific functionality. For the speech input, suggests utterances and answers were set in the skill building interface Amazon provides in its Developer Console.",
      "technologies": [
        "amazon-alexa",
        "javascript",
        "node.js"
      ]
    },
    {
      "project_name": "Theia",
      "likes": 17,
      "submitted_to": "PennApps XIV",
      "winner": true,
      "created_by": "https://devpost.com/mcgomez",
      "description": "Cheap eye exams for third world countries\n\nProviding low cost and eyecare in third world countries can be expensive and require professionals. We want to enable as many people to receive eye care even if they can't afford it or do not have access to an optometry test..\n\nt walks a user through an vision acuity exam and provides an approximate prescription for corrective lenses.\n\nWe developed an iOS application which takes the user through the test, as well as a cardboard housing that holds the phone in the correct lighting and distance. The box is simple to make and can be built by anyone simply with scrap cardboard, scissors, and a ruler (the template is available on our site).",
      "technologies": [
        "ios"
      ]
    },
    {
      "project_name": "PepperPay",
      "likes": 10,
      "submitted_to": "TechCrunch Disrupt SF 2016",
      "winner": true,
      "created_by": "https://devpost.com/davei1",
      "description": "The fun way to buy goods from your favorite retailers.\n\nA few nights ago, as Dave was wondering home after a late-night hack session at the office, he wanted to stop by Walgreens to pick up a fresh tube of toothpaste. He picked up one single item from their easily laid out shelves and got in line. His stay was only beginning. After two arguments between patrons and cashiers and a very slow person, Dave finally walked out of the store 30 minutes after he arrived. He knew there had to be a better way to purchase simple goods. It's 2016 for god-sakes!\nFor years, stores such as Target, Banana Republic, and CVS have been pouring millions of dollars into research about ways to better layout their stores. However, there is one common problem that has yet to be solved: the lag experienced when it comes time to check out.\n\nPepperPay makes checking out of your favorite store easier. It starts at the store, by being asked if you need any help with your store visit. It then allows you to buy the goods you've picked out, without having to deal with a cashier. All you have to do is hold up each item in front of the camera. It will then use image recognition (courtesy of IBM Watson) to figure out which item it is. It will then match that against a database to let you know the price. Without even taking out your wallet, you can then press a button to log into PayPal and pay. This process reduces the average checkout time by over 50%. Built with love by Team Croissant.\n\nWe started by designing a simple flow for checking out different items. We then built a customized web page for the tablet display on the Pepper robot (courtesy of SoftBank). After learning the different gesture and other functionality using Choregraphe, we decided to go with the JavaScript wrapper for the Python API. This enabled us to quickly iterate on different robot control ideas. While that was being worked on, Adam tapped into the Watson Image Recognition API for taking the pictures from Pepper and converting them to specific items that we could put a price on. Lastly, we integrated PayPal via Braintree into the web page and back end, for completing the transaction.",
      "technologies": [
        "angular.js",
        "braintree",
        "ibm-watson",
        "javascript",
        "node.js",
        "paypal",
        "pepper",
        "softbank"
      ]
    },
    {
      "project_name": "Panny",
      "likes": 8,
      "submitted_to": "TechCrunch Disrupt SF 2016",
      "winner": false,
      "created_by": "https://devpost.com/xbili",
      "description": "Intelligent virtual flight attendant that provides travelers with a personalized and comfortable flight experience\n\nAn intelligent virtual flight attendant that provides travellers with a personalized and comfortable flight experience built with Panasonic InFlight API.",
      "technologies": [
        "panasonic-avionics",
        "react.js"
      ]
    },
    {
      "project_name": "Smart Traffic",
      "likes": 13,
      "submitted_to": "Fall HackMTY 2016",
      "winner": false,
      "created_by": "https://devpost.com/david694",
      "description": "Genetic algorithm to increase traffic lights efficiency.\n\nThe great algorithm of natural evolution and the problems that traffic in all cities represent (economic losses, environmental pollution, and social wellbeing, among others).\n\nIt generates a random distribution of cars which serve to test traffic lights from an intersection that are controlled by a set of instructions, and these instructions are constantly optimized with a genetic algorithm.\n\nWe developed a program with processing that uses a set of algorithms made by us and inspired by previous works from other researchers.",
      "technologies": [
        "java",
        "processing"
      ]
    },
    {
      "project_name": "autochime",
      "likes": 11,
      "submitted_to": "CHIMEHACK 3",
      "winner": false,
      "created_by": "https://devpost.com/yuwilbur",
      "description": "Autochime automatically reacts to violence, and records audio evidence while notifying trusted parties\n\nThis project is inspired by alarming statistics showing low justice rates related to violence against women. What resonated with us is the low investigation rate. Most of the cases are not looked at because of the lack of evidence which we thought is the first step to achieving justice. We want to minimize actions required by victims as much as possible which lead us to a smartphone application that can automate violence detection, gather evidence, and request for help.\n\nAutochime is a convenient way to automatically record evidence for sexual or physical assaults. In its initial state, it will look for certain predefined, and later trained, patterns which are associated with incidents of abovementioned type. Once its matches shaky movements and high volume sounds or specific user-customized keywords, it will trigger an alarm which simultaneously starts a recording procedure. This will be sent together with a GPS location to trusted parties, such as friends or family, via SMS. The message also includes a direction to a website where a log of incidents is journaled, upon which they decide to notify authorities or not. \nThe recordings can be later used to take legal action, identify the offender, and bring him/her to justice. This disrupting approach is supposed to encourage women to report as they have credible evidence to rely upon. Furthermore, it may stop violation on spot or even avoid the assault to be happened, as the awareness of the app among potential offenders may raise the anxiety to be caught.\n\nWe begun by putting all of our ideas on the table. Some teammates suggested connecting friends with victims, some suggested integrating with Facebook, and some suggested using the phone’s sensors. We pooled our ideas together and AutoChime was born. Only two of us have Android experience so it was a great learning experience for all of us. We kept the workflow as parallel as possible by writing down all the major tickets on the whiteboard and letting individuals pick which tickets they are most interested in. In the end, all major features were implemented (Albeit, there will always be hacks to mask the bugs as this is a Hackathon).",
      "technologies": [
        "android",
        "java"
      ]
    },
    {
      "project_name": "Hinder",
      "likes": 8,
      "submitted_to": "Android Summit Hackathon",
      "winner": true,
      "created_by": "https://devpost.com/kattim",
      "description": "Find yourself a home within your means with the Tinder of home buying app and survive the Housing Bubble Crash!\n\nThe Housing Bubble Crash in 2008 impacted many Americans who did not know what houses they could afford. We wanted a fun way for people to browse through houses but also learn what houses they can buy within their means.\n\nThe app is like Tinder. Swipe right or left to like or dislike a house. For the houses you like, you can see which houses you have matched with and have been rejected by. For your rejections, you can see why you did not qualify for a house, based on your inputted income and credit score.",
      "technologies": []
    },
    {
      "project_name": "Darcy's World Domination: Shar-Animals",
      "likes": 3,
      "submitted_to": "UHack Tasmania 2016",
      "winner": false,
      "created_by": "https://devpost.com/daniel-k-richardson",
      "description": "Darcy's World Domination Hacktastic Spectacular\n\nThe inspiration for this app came from our appreciation for our local computing society mascot, Darcy Rooster, and a common love of wildlife. We saw an opportunity for an improvement within the wild animal spotting community. With a single, easy-to-use iOS application, users will be able to efficiently and effectively organise wildlife images into a collection, allowing the user to easily share images, and also gain more knowledge about the wildlife in a selected image. Another positive outcome we saw, the app could collect data for researchers. They would be able to make uses of the possibly large data set by accessing and downloading from the site in hopes of assisting in research.\n\nThis is done through having two core parts to the project, the iOS application and the website.\nThe iOS application provides the core function to our large demographic market segments which consists of tourists and wildlife enthusiasts. This application is the way in which users collect, share and gain more information to educate themselves and others about the wildlife that they are finding at their particular locations. This is done by the users opening the application and they are given a menu screen in which they have two options. The first is \"View Collection,\" this view takes them to a collection or gallery of images. This has the ability to filter or share selected images to others. Selecting a single image itself from the view will take you to the photo screen that will show you a full screen view of the image and more information about about the animal that has been found in the image. The second is \"Add a Photo\" which will have a pop-up window that will allow either a selected pre-existing image from the gallery or to take a new photo. \nThe Website provides the core information for our second target market (researchers), who may be interested in accessing the information that is collected and stored by the iOS application through a downloadable file to access the open source database.\n\nThe iOS application was made using SWIFT\nThe website HTML, PHP, CSS, MYSQL (for database integration)\nThe database is MYSQL",
      "technologies": [
        "css",
        "html",
        "mysql",
        "php",
        "swift"
      ]
    },
    {
      "project_name": "Beat the Box!",
      "likes": 5,
      "submitted_to": "Hack the Heat",
      "winner": true,
      "created_by": "https://devpost.com/bb20basketball",
      "description": "Use your hands, face, eyes or a sticky note to clear all the boxes that pop up on screen.\n\nMy inspiration came from learning how to use Cascade Classifiers which allowed me to do cool things such as hand, face, and eye detection. Over the past few months, I have really plunged deep into OpenCV and the different applications of it.\n\nBeat the Box! is a simple concept. Use your \"controller\" to clear the box from the screen by overlapping your \"controller\" with the box. The \"controller\" is can be your hand, eye, face, or a yellow sticky note. The boxes can move if desired and there are different times you can choose from(i.e. 30, 40, 50 seconds). After time runs out, a score is calculated based on your \"Boxes Per Minute\". If you happen to earn the top score in your controller, the high score will be updated on the description page. See the YouTube video to see it in action!",
      "technologies": [
        "opencv",
        "python",
        "wxpython"
      ]
    },
    {
      "project_name": "3DModelResizer",
      "likes": 3,
      "submitted_to": "Hack The 6ix",
      "winner": true,
      "created_by": "https://devpost.com/AmyDZeng",
      "description": "A web service to perfectly and dynamically fit 3D modeled props to your unique dimensions and needs.\n\nThere's nothing cooler than seeing your favorite iconic characters coming to life, and we wanted to help bring that magic to 3D printing enthusiasts! Just starting off as a beginner with 3D modeling can be a daunting task -- trust us, most of the team are in the same boat with you. By building up these tools and automation scripts we hope to pave a smoother road for people interested in innovating their hobbies and getting out cool customized prints out fast.\n\nUsing Blender's API and a whole lot of math, we've created a service that allows you to customize and perfectly fit 3D models to your unique dimensions. No more painstaking adjustments and wasted 3D prints necessary, simply select your print, enter your sizes, and download your fitted prop within a few fast seconds. We take in specific wrist, forearm, and length measurements and dynamically resize preset .OBJ files without any unsavory warping. Once the transformations are complete, we export it right back to you ready to send off to the printers.",
      "technologies": [
        "blender",
        "css",
        "html",
        "javascript",
        "jquery",
        "math",
        "python"
      ]
    },
    {
      "project_name": "LeapDrop",
      "likes": 14,
      "submitted_to": "InOut 3.0",
      "winner": true,
      "created_by": "https://devpost.com/umanghome",
      "description": "LeapDrop is a hack that allows the geek to drag a webpage from one computer and drop it to another computer.",
      "technologies": [
        "android",
        "chrome",
        "java",
        "javascript",
        "leap-motion",
        "node.js",
        "socket.io"
      ]
    },
    {
      "project_name": "Talking WhatsApp",
      "likes": 12,
      "submitted_to": "Open Source Hack",
      "winner": false,
      "created_by": "https://devpost.com/carsonip",
      "description": "Talking instead of typing WhatsApp messages with pure voice commands. No phones needed. Suits lazy guys best.\n\nnspired by the convenience provided by Amazon Echo. Sending and reading messages without touching your phone, real quick.\n\nSend and receive WhatsApp messages on behalf of the user through voice commands.\n\nWe used Python with Flask on our server, Node.js on Amazon Lambda, deployed on Heroku.",
      "technologies": [
        "amazon-alexa",
        "amazon-lambda",
        "flask",
        "heroku",
        "node.js",
        "python"
      ]
    },
    {
      "project_name": "Watch With World",
      "likes": 11,
      "submitted_to": "MLH Prime Spring Finale 2016",
      "winner": true,
      "created_by": "",
      "description": "React to videos in real time with the world\n\nWWWorld's inspiration came from the possibilities of video, twilio's sync api, and the power of iconographic reactions to create an engaging and immersive user experience. Our inspiration drew from sources such as Periscope's live viewing feature and Facebook Live's reaction series (love, wow, haha, sad, angry) in the creation of WWWorld, a Chrome Extension.\n\nWWWorld is a chrome extension that allows users to react to the same video in real time. It utilizes Twilio's Sync API to synchronize video play time between users. Users can then express reactions playfully and intuitively as the video plays with WWWorld's emoji's.\n\nWWWorld was built with rapid prototyping using Sketch App and Adobe Illustrator, and was then further implemented on the Front-End using HTML, CSS, and JavaScript.\nThe Front-End is supported by the Twilio Sync Javascript SDK for syncing live video and live reactions. Authentication is handled by a node backend issuing JWT tokens.",
      "technologies": [
        "adobe-illustrator",
        "chrome",
        "css",
        "html5",
        "javascript",
        "jquery",
        "json",
        "sketch-app",
        "twilio-sync"
      ]
    },
    {
      "project_name": "PetControl",
      "likes": 9,
      "submitted_to": "MLH Prime Spring Finale 2016",
      "winner": true,
      "created_by": "",
      "description": "PetControl enables you to keep an eye on your pet when you're away, and find it when it's lost using a new technique.\n\nAdam had the dream to create this application for months and as his last hackathon, he decided to peruse it.  It started as a simple way to use QR codes on dog tags to get alerted when someone finds your lost dog, it turned into much more.\n\nThere's two goals of PetControl, the first is the original idea of using a QR code to keep track of pets. Current dog tags can hold small amounts of information, maybe a name and an address.  By using PetControl's QR tags pet owners can set large amounts of information.  The owner can tell the person who finds it the dogs name, the owners name, the phone number, address, and special notes about the pet.  By doing this it allows the pet to get home to the owner faster and safer.\nThe second part of PetControl came after, instead of only being able to be used when a pet is lost, PetControl can be used any time of the day to help manage and take care of your beloved animal friend.  With widgets in a dashboard on our website, pet owners can log in and view a live video of their pet with their Nest Camera, order more food for their pet using the eBay API, or mark their dog as lost so they are immediately notified whenever someone scans their dog's tag.\nNot everyone has a QR code reader, or a smartphone, that's why we used Twilio to send a picture of the tag to a number and it act the same as the app.\n\nThe back end was all developed using Python, combined with Flask we were able to create a fully functioning website complete with accounts, pets, dashboard, and numerous other features (including Twilio support). The mobile app was developed for Android and is complete with tag scanning functionality, pet lookup, and pet management.",
      "technologies": [
        "android",
        "css",
        "flask",
        "html5",
        "javascript",
        "materializecss",
        "python"
      ]
    },
    {
      "project_name": "Chess++",
      "likes": 7,
      "submitted_to": "MLH Prime Spring Finale 2016",
      "winner": true,
      "created_by": "https://devpost.com/GlennRen",
      "description": "A fun and interactive way of experiencing chess.\n\nChess is a game that has been around for centuries, and remains widely popular even in today's digital age. Whereas games today are utilizing increasingly sophisticated technology in order to provide players, both experienced and inexperienced, with a more modern look and feel to them, the chessboard has remained largely the same throughout its history. As a result, we decided to combine the traditional game of chess with a modernized interface in order to increase its appeal to a new, modern generation.\n\nThe Chess++ board has the functionality of a traditional chess board, and can be used to play a traditional game of chess. However, the Chess++ board is also connected to a website which displays the current positions of the pieces on the board. Not only does the Chess++ board send information to the website, but it also takes in information from the website and displays it on the board. The Chess++ board includes LED lights below each square, and these lights can be used to display to inexperienced users the possible actions that a certain piece could perform. Once a player picks up a piece from the Chess++ board, it registers that a user has picked the piece up and displays the possible actions the piece can take. This functionality is aimed to assist new users in quickly grasping the rules of chess and to allow them to enjoy the game faster.\n\nThe Chess++ board utilizes cloud technology in conjunction with the Raspberry Pi and the Arduino Uno in order to accomplish its functions. The board itself is built with conductive tape, which completes a circuit when a piece is placed on the grid  The grid is connected to the Arduino, which is the conduit between the website and the board itself. In addition, the Raspberry Pi is used to turn on the proper LEDs, which are situated underneath the board.",
      "technologies": [
        "arduino",
        "c-sharp",
        "css3",
        "hardware",
        "html5",
        "javascript",
        "joint.js",
        "python",
        "raspberry-pi"
      ]
    },
    {
      "project_name": "Scan 'n' Grab",
      "likes": 4,
      "submitted_to": "School's Out Hackathon 3",
      "winner": true,
      "created_by": "https://devpost.com/ReedvEggleston",
      "description": "Network administrator's new tool to scan for open ports, while looking for outdated software.\n\nIn cyber security, there are many well know tools that exist. I decided that I wanted to understand the underlying mechanics of making a scanner from the ground up, and add more functionality. Hence, banner grabbing was included, an the Scan 'n' Grab was born.\n\nThe Scan 'n' Grab is a program that knocks on the \"doors\" of a computer looking for open \"doors\" and \"doors\" that respond with information. A network administrator can use this information to audit what services a server is running, and what version some of the software is running. It can even catch the occasional backdoor.",
      "technologies": [
        "python"
      ]
    },
    {
      "project_name": "PocketShop",
      "likes": 7,
      "submitted_to": "UNIHACK 2016",
      "winner": false,
      "created_by": "https://devpost.com/joshparnham",
      "description": "Connecting customers and shops instantly\n\nPersonal painpoints, there currently isn't a simple way to find exactly what we're looking for quickly at shopping centres. For example, a customer who wants a pair of red shoes who doesn't have time to browse all the different shoe shops and their catalogues to find that perfect pair.\n\nThe app is an instant-messaging platform that matches customers to stores after customers place Buying Requests – items that they are looking to purchase. Retailers that stock that category of item receive a notification and are able to converse with the customers about the items in their stock that may be of interest.\n\nThe main application is an iOS app written in Objective-C. This talks to a backend server written in Node.js, hosted on Heroku and backed by a MongoDB datastore. We also built an example retailer-management web application in React which allows the retailers to converse with the customers in the app.",
      "technologies": [
        "iphone-sdk",
        "javascript",
        "json",
        "mongodb",
        "node.js",
        "npm",
        "objective-c",
        "react",
        "sinch"
      ]
    },
    {
      "project_name": "The Magic Hand",
      "likes": 9,
      "submitted_to": "UNIHACK 2016",
      "winner": true,
      "created_by": "https://devpost.com/OliviaOng",
      "description": "It does make a difference.\n\nNowadays, people cannot live without mobile phones and we cannot interact with people without touching a capacitive screen. However, imagine you are a disable person with no hand or paralysis and have to use the huge, ugly and special mobile devices which makes you feel special already, how would you feel? Therefore, our design is trying to help those disable people to use modern capacitive screen mobile phone with a simple and convenient phone cover film which can touch any points of the screen instead of human fingers.\n\nThe Magic Hand is a film which can be attached to any capacitive screen to help disable people to use the phone. It works as a output and takes instructions from any sort of input sensors, eg, LDR, Muscle sensors, BCI and Voice Control.\n\nWe built a prototype of our design to interact a game called piano tiles which has high requirement on dexterity and reaction time. We used Light-Dependent Resistors (LDR) to detect the brightness of the tiles at certain points. Arduino board was used to process and control the signals at outputs. Furthermore, the to simulate human fingerprints, we used relay, resistors, aluminium foil and 3D-printing to trigger the capacitive screen.",
      "technologies": [
        "3dprinting",
        "arduino",
        "circuit-design",
        "ldr",
        "simplify-3d",
        "soildworks",
        "soldering"
      ]
    },
    {
      "project_name": "DOChuman",
      "likes": 4,
      "submitted_to": "UNIHACK 2016",
      "winner": false,
      "created_by": "https://devpost.com/chuaccheng",
      "description": "Live patient information for doctors\n\nDOChuman, pronounced \"documen\" (in line with @developersteve's import-ant puns) is live patient information for doctors.\nHow it works is that we give each patient a unique identifier in the form of Estimote. When a doctor walks near a patient, the patient's information automatically pops up on the doctor's iPad.\n\nWe did our UI on Sketch, coded in Swift for the iOS app and linked the two.",
      "technologies": [
        "estimote",
        "sketch",
        "swift"
      ]
    },
    {
      "project_name": "Pyraminx Scheme",
      "likes": 5,
      "submitted_to": "UNIHACK 2016",
      "winner": true,
      "created_by": "https://devpost.com/McSimp",
      "description": "An app that shows you how to solve a pyramix after reading in its configuration with image processing\n\nWe found that there were no apps available for the particular type of puzzle that we were trying to solve, so we decided to make one for ourselves! The idea is the make it educational to allow people to learn how to solve the puzzle, and not just mindlessly rotate things around.\n\nYou show the app each side of your pyraminx and it will automatically read in the colours and determine the configuration of the puzzle. It will then show you the moves necessary to solve the puzzle.\n\nWe used OpenCV for the image processing and Unity for the 3D rendering.",
      "technologies": [
        "c#",
        "c++",
        "opencv",
        "unity"
      ]
    },
    {
      "project_name": "Objective-She",
      "likes": 8,
      "submitted_to": "Spectra",
      "winner": false,
      "created_by": "https://devpost.com/yamag104",
      "description": "Objective: She feels empowered and encouraged into Tech\n\nWomen are an invaluable part of the tech industry but only account for 25% of workers in the industry. We wanted to create an app that would encourage women into technology by helping them connect with women who were already in the field.\nIt took a while for me to find my passion in Computer Science because I did not have any mentor to guide me through my interests. I was intimidated by the field and had an impression that it was mostly for men. This experience inspired me to create this app to connect women in Tech and inspire and welcome young girls into this field.\n\nObjective-She allows users to create a profile and post information about themselves. They are then shown potential mentors/mentees and if they think they would be a good match with that person they swipe to the right to signify their interest. If a mentor/mentee match, they are then able to talk to each other and talk about their experiences in Tech. \nWe wanted to make it as simple as possible for women to mentor and find mentors, so we created and iPhone app that has a very simple interface to look at mentors or mentees profile and connect with them instantaneously.\n\nThis app is written in Swift using Xcode. We utilized Koloda framework to simplify the implementation of swiping cards motion along with Parse for back-end.\nWe used Sketch and InVision to create the mockups.",
      "technologies": [
        "ios",
        "sketch",
        "swift",
        "xcode"
      ]
    },
    {
      "project_name": "Idiomatic",
      "likes": 8,
      "submitted_to": "Spectra",
      "winner": true,
      "created_by": "https://devpost.com/lindsbro",
      "description": "Learn idioms in English\n\nIdioms are common sayings that native languages often use. For English language learners and students with autism, idioms are very challenging. Idioms often have no clear relation to their meaning and are not literal, and thus hard to learn and understand. English-as-a-second-language learners and students with autism can have better conversations and more deeply engage with native English speakers when they have idioms in their vocabulary.\n\nThe learner can search for a word or phrase. Idiomatic will return an idiom, its meaning and (will include) a relevant YouTube video. This is valuable for the learner, as they will find the meaning and have a visual reference for the idiom.\n\nSet up involved:\nCreate Heroku project\nCreate Github repo\nSet up Ruby on Rails \nInstall React\nCreate database in SQLite 3\nOur project involved:\nWireframing\nResearch on idioms\nDesign process\nCompiled idiom data into a spreadsheet (idiom, meaning).\nPut compiled idioms into database\nLots of coding!",
      "technologies": [
        "css",
        "heroku",
        "html",
        "javascript",
        "love",
        "react",
        "ruby-on-rails",
        "sketch",
        "sqlite"
      ]
    },
    {
      "project_name": "Urban Arcade",
      "likes": 8,
      "submitted_to": "",
      "winner": false,
      "created_by": "https://devpost.com/walkman1915",
      "description": "Classic arcade game + GPS technology = Running around neighborhoods, dodging ghosts, and collecting dots in real time\n\nAt the inaugural hackathon HackGT@FIB, a cooperative event between Georgia Tech and the Facultat d'Informacion de Barcelona held in Barcelona, Spain, the theme was \"Disrupting Social Accessibility\". Our team brainstormed many ideas, from utilizing the growing sharing economy to catering to the loneliness of the elderly, but in the end we decided to encourage physical activity, outdoor exploration, and plain old fun. After the release of Pokemon Go, we realized how exciting real-time GPS tracking could make a game, and what better medium than through the iconic and timeless Pac-man?\n\nUpon navigating to the website on a mobile device (Chrome works best), a map appears, centered on the user. The Pac-man avatar is at the user's location, and several dots are scattered around along sidewalks and walkways. Upon pressing \"start\", the red ghost appears and begins chasing the user, and the sound effects begin. The user must run around and collect all the dots to get as many points as possibly before being caught by the ghost!\n\nWe split the project into three main parts: the ghosts, Pac-man, and the dots. All three tasks utilized the Google Maps API in different ways.\nThe ghosts use overlays to display their sprites on the map, and to chase Pac-man they continuously move towards him, ignoring walls and terrain. When the collide, the game ends.\nPac-man was created by using a Marker object and continuously moving around the screen as the user moves.\nThe dots use the Google Maps Directions API to create a route from the origin (where the player began) to a location on a grid, determined by the desired map size and dot density. The nearest sidewalk or pathway to this dot is then determined, and this new location is where a corresponding dot is placed. When dots are walked over, they disappear and give the user points.",
      "technologies": [
        "css3",
        "google-maps",
        "html5",
        "javascript"
      ]
    },
    {
      "project_name": "Places Now",
      "likes": 9,
      "submitted_to": "HackGT@UPC 2016",
      "winner": false,
      "created_by": "https://devpost.com/Cadenjiang",
      "description": "The beach in Barceloneta is crowded? That restaurant has a 1 hour wait? You’ll know before you even leave your door.\n\nGoogle Maps is on its own pretty slow; getting information goes through a long process through Google Map Maker, which then goes through a Google review process. Users are unable to access real-time, reliable information about various public spaces. What if a tourist attraction is closed off due to renovations? Or there is a great sale at your favorite shop?\n\nPlaces Now hopes to solve this problem by crowdsourcing information about individual locations at specific times.\n\nWe built it using Android Studio, the Google Maps API, and Firebase.",
      "technologies": [
        "adobe-illustrator",
        "android-studio",
        "firebase",
        "google-maps"
      ]
    },
    {
      "project_name": "nofoodwasted",
      "likes": 54,
      "submitted_to": "Islamabad Civic Hackathon 2016",
      "winner": true,
      "created_by": "https://devpost.com/cwtausif",
      "description": "No Food Wasted is a platform to tackle food wastage and feed underprivileged.",
      "technologies": [
        "java",
        "mysql",
        "php"
      ]
    },
    {
      "project_name": "Bagh Bagh",
      "likes": 13,
      "submitted_to": "Islamabad Civic Hackathon 2016",
      "winner": true,
      "created_by": "https://devpost.com/ayeshafazal",
      "description": "Building gardening communities through plant, information and space share to improve environment and emotional health",
      "technologies": [
        ".net",
        "android",
        "asp.net",
        "bootstrap",
        "community",
        "csharp",
        "css3",
        "html5",
        "jquery",
        "mockup.io"
      ]
    },
    {
      "project_name": "Boops Boops Swoops",
      "likes": 2,
      "submitted_to": "",
      "winner": false,
      "created_by": "https://devpost.com/ndbroadbent",
      "description": "An augmented reality game for iOS where you tilt your phone to fly through hoops\n\nhttps://en.wikipedia.org/wiki/Boops_boops\n\nIt's a very simple game. There's a basic augmented reality mode where it uses your phone's camera.",
      "technologies": [
        "ios",
        "scenekit",
        "swift"
      ]
    },
    {
      "project_name": "Goldilocks Browse",
      "likes": 13,
      "submitted_to": "",
      "winner": false,
      "created_by": "https://devpost.com/reperry",
      "description": "Browse to find that \"just right\" product by incrementally changing attributes such as height, width, roundness, etc.\n\nWhen I'm shopping, sometimes I know exactly what I want, and there are so many products out there that I am sure it exists somewhere. This inspired me to create a better way to search through existing products to find exactly what I wanted and get that satisfaction of receiving a custom-made item, without the time and cost of having something custom made.\n\nGoldilocks browse shows a set of products that are all very similar, but differ in a few key attributes. The interface shows one item at a time. The user can then move through the products by indicating that they like the product but want it to be taller, shorter, wider, rounder, etc. for predefined key attributes of a product. The result is a view that looks like a stop-motion film as one product is replaced with the next.",
      "technologies": [
        "javascript",
        "python"
      ]
    },
    {
      "project_name": "Cornhole",
      "likes": 5,
      "submitted_to": "",
      "winner": false,
      "created_by": "https://devpost.com/MikeFesta",
      "description": "Virtual Cornhole in the HTC Vive\n\nWe have a backyard scene from an existing project and thought it would be fun to play yard games out there.\n\nRealistic bean bag physics",
      "technologies": [
        "blender",
        "unity"
      ]
    },
    {
      "project_name": "Chaotic Magnetic Pendulum",
      "likes": 9,
      "submitted_to": "McGill Physics Hackathon",
      "winner": false,
      "created_by": "https://devpost.com/felleg",
      "description": "Simulation of the behaviour of a magnetic pendulum to demonstrate the butterfly effect.",
      "technologies": [
        "css",
        "html",
        "javascript"
      ]
    },
    {
      "project_name": "DecorNet",
      "likes": 10,
      "submitted_to": "",
      "winner": false,
      "created_by": "https://devpost.com/cungtv",
      "description": "Turn your phone into an interior decorator\n\nSo you bought that couch. Now which coffee table will go with it? How about some accent chairs and wall art? Can you pick the right combinations? What would it take to turn everyone into a decorator?\n\nYou can take picture of things in your home and it will recommend what else that you can buy from Wayfair that will go along with it\n\nWe do it in two steps. First step is powered by a deep neural network (trained over the weekend). It identifies closest matching product from Wayfair with the image you snapped. In the second step we recommend items from other related classes that we have compiled based on customer buying patterns.",
      "technologies": [
        "android",
        "deep-leaening",
        "gpu",
        "hadoop",
        "hive",
        "python",
        "theano"
      ]
    },
    {
      "project_name": "mapster",
      "likes": 3,
      "submitted_to": "",
      "winner": false,
      "created_by": "https://devpost.com/_ace",
      "description": "Bring Maps to Slack!  Mapster the helpful map bot.",
      "technologies": [
        "javascript"
      ]
    },
    {
      "project_name": "Follow me",
      "likes": 10,
      "submitted_to": "Smart Transportation & Energy Hackathon",
      "winner": false,
      "created_by": "https://devpost.com/WojciechHupert",
      "description": "Autonomous drone assistant\n\nReverse drone delivery (drone takes you to the location rather than delivering goods)\nReverse principle of the current implementation of the 'follow me' drone which follow subject for the purpose of cinematography etc.\n\nFOLLOW ME is a “reverse delivery” drone system that solves the problem to get most efficiently and without any “congestion” to your point of interest.\nThat’s possible, because its backend server manages all the routes and times in a specific areal.\nWe wanted to provide a solution to problems at the 'last mile' by providing an infrastructure of fully automated, independent, self-maintaining (autonomous) drones. In this use case we created a system for delivery trucks that need an assistance in getting from the entrance gate to a specific location of the business park. Drones are aware of each truck and can manage the traffic of multiple vehicles they provide assist to.\nDrones are equipped with an infrared sensor in order to be aware of the proximity of a vehicle they assist. They also maintain an altitude of 3m above the ground thanks to a laser sensor.\n\nWe used low cost drones equipped with infrared and laser sensor, HTML interface for dispatch kiosk, KML route planing (Keyhole Markup Language)\nHARDWARE:\n•Set of multiple Follow Me Drones, equipped with a RGB-Display, 6-axis stabilization gyro, GPS,\n  laser ground distance sensor,\n  infrared sensor to stay by the customer (engine heat or facial heat)\n•SkySense.co fully automatic “home base” recharging drone pads\nSOFTWARE:\n•Frontend: App & Web-Service (HTML)\n•Backend: Any rented server (e.g. Amazon Web Services)\n•qGroundControl open source drone control software add-in",
      "technologies": [
        "3dr-gps-chip",
        "ar-drone",
        "drone",
        "esc",
        "esc-quatro",
        "google-maps",
        "gps",
        "here-navigation",
        "html",
        "kml",
        "qground-control",
        "sky-sense",
        "taoglas",
        "touchscreen",
        "wifi"
      ]
    },
    {
      "project_name": "Locatio",
      "likes": 9,
      "submitted_to": "Smart Transportation & Energy Hackathon",
      "winner": true,
      "created_by": "https://devpost.com/a-j",
      "description": "A revolutionary solution for shipment planning\n\nKuehne & Nagel's real problem of planning huge amounts of shipments all around the world\nproblem of sending different couriers to the same location to ship goods to customers which is a waste of resources\nproblem of inconsistency when handling location data provided by customers\n->Huge Optimizations Possible!\n\nnormalize location data from different customer sources\nprovide additional meta information about courier routes and destinations (minimal street width, maximal curviness, building information, opening times, ...) using the here maps API\nhelps Kuehne & Nagel to plan shipments with route information and offers consolidations instead of using multiple half empty Excel files with sometimes 200.000\n\nThe data was first cleaned and normalized using Python (with numpy and pandas).\nAt the same time we developed a Web application to simplify the way the end user interacts with our data.\nThe Web app is served via a Flask Server with a beautiful Front-End.\nWe used an informal Scrum approach by having multiple iterative sync meetings and brainstorming sessions.",
      "technologies": [
        "ajax",
        "big-data",
        "css",
        "flask",
        "here-api",
        "here-maps",
        "html",
        "javascript",
        "jqeury",
        "jupyter",
        "materialize",
        "numpy",
        "pandas",
        "python"
      ]
    },
    {
      "project_name": "KaraokeGenerator",
      "likes": 9,
      "submitted_to": "EngHack",
      "winner": false,
      "created_by": "https://devpost.com/Nhat",
      "description": "Convert your music file into an instrumental track with auto generated lyrics\n\nTracking down instrumental version of songs to use for karaoke can be extremely difficult and frustrating. For some rare songs, it can also be difficult to find accurate lyrics for them, forcing you to to either transcribe the audio manually, or give up. By providing a service that automatically generates an instrumental song file and its corresponding lyrics for you, it is no longer necessary to waste your time searching.\n\nKaraoke Generator allows the user to quickly and efficiently get the instrumental track and lyrics for a song of their choice. All that's required by the user is to upload the chosen music file, and the application will remove the vocal track, convert the vocal track into lyrics and return the track without vocals.\n\nThe front end that the user interacts with was made using Android. After the user selects a song, the song is uploaded to a Django server for file processing. Once the audio file is uploaded, signal processing is done in python to strip the vocal tracks from the song. In addition, lyrics are generated by breaking up the song into multiple files, and sending it to Google's speech-to-text API. Once processing is complete, the lyrics are returned to the android app, along with a path to the instrumental file on the server. Finally, the file is retrieved and the user can use the new instrumental track as they please.",
      "technologies": [
        "android",
        "django",
        "java",
        "python"
      ]
    },
    {
      "project_name": "Settler",
      "likes": 7,
      "submitted_to": "We Hack Too powered by Microsoft - For Girls",
      "winner": false,
      "created_by": "https://devpost.com/shaza",
      "description": "Connecting and empowering refugee and immigrant communities\n\nAs residents of Atlanta, we were inspired by the diverse immigrant and refugee communities we saw in our city. We realized the impact a community has on the well-being of a person, and we wanted to create a way for people to find a support system, especially after leaving their friends and family in a different country.\n\nSettler is an app that shows you people who have the same origin and current city as you. If you moved from Aleppo, Syria, but now reside in NYC, the app will add you to the closed community of others just like you. Members can message those on the same list, and create groups for different interests. There is also a bulletin board, in which anyone on the list can post listings. This feature is especially helpful to share things like job opportunities, ESOL classes, event listings, or want ads.",
      "technologies": []
    },
    {
      "project_name": "Kitchen IoT",
      "likes": 4,
      "submitted_to": "EngHack",
      "winner": false,
      "created_by": "https://devpost.com/MunazR",
      "description": "Control your kitchen appliances and automatically steep your tea\n\nWe want to make our boring kitchen appliances smarter by allowing us to control them more easily .\n\nIt automatically turns your appliances on/off when required. Can also be used to steep your tea.\n\nWe used an arduino microcontroller. An AC relay is used to control power to appliances. A servo motor is used to steep your tea.",
      "technologies": [
        "arduino"
      ]
    },
    {
      "project_name": "HighGround.NYC",
      "likes": 2,
      "submitted_to": "Hack Red Hook 2016",
      "winner": false,
      "created_by": "https://devpost.com/AAAndyAn",
      "description": "A resiliency program for minimizing water damage to automobiles in flood-prone urban areas.\n\nThis is a known issue. Insurance Auto Auctions, which handled 40% of the wrecks that went to the salvage auction market dispatched 400 tow trucks and leased huge holding facilities even before Sandy hit. Further damage done by unwitting buyers purchasing illegally disguised flood cars in the second hand market.\nThe city cannot just provide an evacuation plan for Red Hook's people, it also needs one for their vehicles.\nSolving this issue was proposed by PortSide NewYork during the Red Hook Hackathon at Pioneer Works, June 2016.  PortSide NewYork is a White House Award-winning, waterfront-oriented non-profit that was very involved in hurricane Sandy recovery work and is now involved in resiliency planning. Their President Carolina Salguero spoke at the hackathon, and our team was inspired to create this app.\n\nHigh Ground NYC is a resiliency program for minimizing water damage to automobiles in flood-prone urban areas. It consists of:\n1) Specially designated emergency flood parking on high ground relatively close to surge areas.\n2) a notification system for when flood parking is in effect- via SMS & physical LED indicators\n3) Real-time sensing of flood parking availability on a per-spot, hype-load basis\n3) An application that guides users to the closest available parking spot in the most efficient way.\n5) an AI \"traffic cop in the cloud\" that intelligently manages the distribution of parking spots in real time.",
      "technologies": [
        "arduino",
        "ember.js",
        "scriptr",
        "twilio"
      ]
    },
    {
      "project_name": "clickBait-Destroyer",
      "likes": 7,
      "submitted_to": "",
      "winner": false,
      "created_by": "https://devpost.com/jssaini07",
      "description": "Destroys facebook clickbait",
      "technologies": [
        "javascript",
        "jquery"
      ]
    },
    {
      "project_name": "OneApp",
      "likes": 10,
      "submitted_to": "Battle of the Hacks v 3.0",
      "winner": true,
      "created_by": "https://devpost.com/xLegoz",
      "description": "Try a new app instantly\n\nEver since Google demo'd Instant Apps at IO this month, we've been intrigued by the possibilities of this technology and curious whether it could work on iOS. Many people have told us that this would be difficult or even impossible, but with some knowledge of react native hot-reloading, we were convinced that this was not only possible, but also the perfect project for a16z.\n\nBased on the user's context, such as GPS location, OneApp prompts the user about apps that could be useful and allows the user to use them immediately. OneApp grabs the relevant app from a server, compiles the code and sends the result directly to the user's phone.  Theoretically, OneApp could process any React Native app in this way.\n\nWe modified React Native by intercepting communication to its server and inserting our own code. We also built a number of modules.",
      "technologies": [
        "mongodb",
        "node.js",
        "react",
        "react-native"
      ]
    },
    {
      "project_name": "Falcon",
      "likes": 11,
      "submitted_to": "Battle of the Hacks v 3.0",
      "winner": true,
      "created_by": "https://devpost.com/andrewilyas",
      "description": "An intelligent semantic full-page internet history browser",
      "technologies": [
        "android",
        "elasticsearch",
        "golang",
        "java",
        "javascript",
        "machine-learning",
        "objective-c"
      ]
    },
    {
      "project_name": "fbash",
      "likes": 8,
      "submitted_to": "",
      "winner": false,
      "created_by": "https://devpost.com/avikjain",
      "description": "Terminal over Facebook Messenger, running continuously as a background process.",
      "technologies": [
        "node.js",
        "npm"
      ]
    },
    {
      "project_name": "Tic Tac Fail",
      "likes": 1,
      "submitted_to": "terribleHack@VHS",
      "winner": true,
      "created_by": "https://devpost.com/bentekkie",
      "description": "A tic tac toe bot behind a rest api that will always loose or tie\n\nHaven't you ever wanted to have something that is more of a failure than you are, well I have one now :P\n\nIt will purposely loose at tic tac toe by chosing the moves that have the most winning outcomes for you -  how considerate :)",
      "technologies": [
        "racket"
      ]
    },
    {
      "project_name": "gimme-keycodes",
      "likes": 3,
      "submitted_to": "",
      "winner": false,
      "created_by": "",
      "description": "get javascript keycodes for every key on your keyboard!",
      "technologies": [
        "css",
        "html",
        "javascript"
      ]
    },
    {
      "project_name": "swip",
      "likes": 14,
      "submitted_to": "Inno{Hacks}",
      "winner": true,
      "created_by": "https://devpost.com/timgrossmann",
      "description": "Connect multiple smartphones to create physical ui experiences",
      "technologies": [
        "css",
        "html",
        "javascript",
        "jquery",
        "node.js",
        "socket.io"
      ]
    },
    {
      "project_name": "Time Traveling for Dummies",
      "likes": 11,
      "submitted_to": "BASEHacks",
      "winner": true,
      "created_by": "",
      "description": "A comprehensive website to take care of all the pesky logistics of time travel.\n\nRecently, I traveled back to 1776 to see Washington cross the Delaware. Bad idea – I nearly died of hypothermia since I, as a shorts-wearing Californian, was not prepared for the New England winter. Even worse, no shops would accept my $3 to buy Doritos! They didn't even have Doritos!!! Who could have possibly known this would happen?!? If only I had thought of those annoying details beforehand...\n\nEnter the day and place to which you plan to travel and the amount of money you will be bringing along, and Time Traveling for Dummies will provide you useful information about weather conditions and the value of your money.",
      "technologies": [
        "css",
        "html",
        "javascript",
        "wolfram-technologies"
      ]
    },
    {
      "project_name": "AROS",
      "likes": 9,
      "submitted_to": "BASEHacks",
      "winner": true,
      "created_by": "https://devpost.com/EthanReid",
      "description": "AROS - Access any file in augmented reality.",
      "technologies": [
        "unity",
        "vuforia"
      ]
    },
    {
      "project_name": "BookBot",
      "likes": 6,
      "submitted_to": "BASEHacks",
      "winner": true,
      "created_by": "https://devpost.com/arjunds",
      "description": "A messaging bot that allows you to access your local library\n\nThe system of going to a library, checking out books, returning books, and placing holds on old fashioned websites is too tedious, so we thought of a way to modernize and automate a library with a chat bot.\n\nThe product, BookBot, lets users and librarians access the library from their phones and search for books, view holds, and more.",
      "technologies": [
        "facebook-messenger",
        "google-book-api",
        "gupshup",
        "javascript",
        "node.js",
        "slack"
      ]
    },
    {
      "project_name": "Snö By GingerPonyPatlettes",
      "likes": 33,
      "submitted_to": "#SlackathonMTL",
      "winner": false,
      "created_by": "https://devpost.com/patricklafrance",
      "description": "Make New Employees Integration Easy\n\nSelon une étude du MIT, 90% des employés décident s'ils veulent rester dans une entreprise au cours des 6 premiers mois.\nLe coût de la perte d'un employé dans la première année est de 3 fois son salaire.",
      "technologies": [
        "botkit",
        "node.js",
        "slack"
      ]
    },
    {
      "project_name": "Taz",
      "likes": 26,
      "submitted_to": "#SlackathonMTL",
      "winner": true,
      "created_by": "https://devpost.com/philavoie",
      "description": "Your notification storm is over\n\nRemove the key frustration of Slack users: Tired of micro-managing unread messages in multiple channels in Slack.\nRecap/digest messages intelligently for users.\n\nUsing a slash command, Taz creates a digest of the unread messages that are:\nTrending based on emoticons\nDirectly or undirectly mentionned to you\nMost talked about topics\n\nHaving the problem clearly defined, our first priority was to brainstorm a backlog of features and figure out which were part of our minimal viable product (MVP) and which were nice to have that we could squeeze in at the end to wow the crowd.\nWe then went on an epic journey to find the brand we would give our bot.  This would in turn give us a better idea how to design, market and bring it some appeal.\nOn a technical note, a Slack command sends data to our Azure web API endpoint.  This endpoint then crunches your Slack data and figures out the unread messages and direct/undirect mentions.  This formatted data is sent as a beautiful digest in the taz bot instant messaging channel.",
      "technologies": [
        "asp.net",
        "azure",
        "c#",
        "css",
        "html",
        "javascript",
        "slack"
      ]
    },
    {
      "project_name": "GROOT",
      "likes": 17,
      "submitted_to": "#SlackathonMTL",
      "winner": true,
      "created_by": "https://devpost.com/Anthony8603V",
      "description": "Plants as team members.\n\nPlants have an amazing impact on worker happiness a healthy workplace. Apart from improving air quality, they've been proven to increase productivity by up to 15% and help reduce stress and anxiety by up to 35%.\nBut plants die. It's also pretty expensive to hire someone to take care of what your employees perceive as a glorified piece of furniture. So we made GROOT.\n\nGROOT suggests plants that are well suited for your specific workspace. It finds an owner for the plants, which they notify when they need to be maintained. It allows your employees to really take control of their environment, form a bond with their workspace, and ultimately end up happier and more engaged.\nBest of all, this happens through natural language discussions through your favorite team messaging platform: Slack! (Almost) No forms to fill. Never boring.\n\nWe've built GROOT on the proven Django/Python web stack. It's deployed on cloud VMs, making horizontal scaling as easy as 1-2-3.",
      "technologies": [
        "django",
        "python"
      ]
    },
    {
      "project_name": "Bet Bot",
      "likes": 9,
      "submitted_to": "#SlackathonMTL",
      "winner": false,
      "created_by": "https://devpost.com/alisterdev",
      "description": "Bet on your favourite sports teams with your colleagues for unlimited office fun!\n\nSports as a central theme. Sports provide a platform to promote cohesiveness between co-workers.\n\nAllows employees to track upcoming games and place bets.\n\nWith bare hands, love and caffeine...",
      "technologies": [
        "botkit",
        "node.js",
        "slack",
        "stattle"
      ]
    },
    {
      "project_name": "PeterBot",
      "likes": 9,
      "submitted_to": "",
      "winner": false,
      "created_by": "https://devpost.com/javpet",
      "description": "CV sending sucks, so I created PeterBot my personal chatbot pitching for jobs",
      "technologies": [
        "chatfuel",
        "facebook-messenger",
        "heroku"
      ]
    },
    {
      "project_name": "The 6ixth Floor",
      "likes": 3,
      "submitted_to": "JAMCHESTER",
      "winner": false,
      "created_by": "https://devpost.com/AlexWallace",
      "description": "Hey look, It's the credits! The game must be over, right?",
      "technologies": [
        "construct2",
        "scirra"
      ]
    },
    {
      "project_name": "Hold Your Gold",
      "likes": 4,
      "submitted_to": "JAMCHESTER",
      "winner": false,
      "created_by": "https://devpost.com/sarahakers",
      "description": "A tower defense, shoot em' up protecting the treasure you've already claimed from the treasure itself.\n\nWe created the game using Unity and VISR VR. The art for the game was created by Sarah and it was programmed by Brad and Alex.",
      "technologies": [
        "maya",
        "unity"
      ]
    },
    {
      "project_name": "Drone Control for Unity",
      "likes": 8,
      "submitted_to": "",
      "winner": false,
      "created_by": "https://devpost.com/leogmaia",
      "description": "A simple control game which can be used on Unity. I'ts an arcade friendly drone control with X-Box like controller.\n\nBy watching drone race videos came the idea of creating a game for that.  However, a drone controller is a little different than console controllers due it's lose axis (normally the Y1 axis).  The idea is to create a lib to be used by normal console controllers on an arcade like drone\n\nIt's present a very rough drone model to be controlled by the x-box game controller.  The movements follow the actual drone controller, despite it take in count the springed Y axis by keeping the altitude control automatic for the player.",
      "technologies": [
        "c#",
        "unity"
      ]
    },
    {
      "project_name": "Citizen Pass - 3rd place Winner",
      "likes": 10,
      "submitted_to": "",
      "winner": false,
      "created_by": "https://devpost.com/Mandy_Chan",
      "description": "A voice control game that is designed to help US residents to prepare for the citizenship exam\n\nCitizen Pass turns Alexa into your own personal study buddy for the citizenship exam. Since Alexa is voice activated, I can still remain physically busy while practicing for my citizenship exam next month in June.\n65,000 people were naturalized in 2014 and there are 40% of NYC residents who were born outside of the US. This skill benefits this group of demography and helps people pass the citizenship exam at ease. Once you pass the exam, try Dr. Speech; another application I built on top of Alexa to help improve your pronunciation.\n\nAn Alexa skill that is designed to help US residents to prepare for the citizenship exam.",
      "technologies": [
        "amazon-alexa",
        "amazon-web-services",
        "chai",
        "css3",
        "html5",
        "javascript",
        "mocha",
        "node.js"
      ]
    },
    {
      "project_name": "Houston Sights",
      "likes": 5,
      "submitted_to": "4th Annual Houston Hackathon",
      "winner": false,
      "created_by": "https://devpost.com/CyberneticOwl",
      "description": "It generates dynamic tours based on the user's preferences explaining the history and information of each sight.\n\nTo innovate a cool, easy, way that moves Houston's tourism and economy forward and serves a growing need to explore local Houston sights.  The SuperBowl, Houston Convention & Visitors Bureau, and other major partners would have a strategic way heighten greater engagement that encourages more tourism and creates commerce in Houston.\n\nHouston Sights connects citizens and visitors to a better experience of all Houston has to offer by generating customized, guided tours based on user preferences and location.  Adaptive tour generation by user search preference:\nHistory\nCulture\nArt\nScience\nFood\nMuseum\nShopping\nPark\nLandmark\nSpiritual\nEntertainment\n\nCoordinated efforts using Java, android studio, google maps, and imaging software.",
      "technologies": [
        "android",
        "android-studio",
        "google-maps",
        "java",
        "xml"
      ]
    },
    {
      "project_name": "SolarMaps",
      "likes": 7,
      "submitted_to": "Powerhouse SunCode Solar Hackathon 2016",
      "winner": true,
      "created_by": "https://devpost.com/petethepig",
      "description": "Visual display for the value of solar on buildings, facilitating a solar marketplace.\n\nThe pain of selling solar and the sorrow of people not believing that solar can save them real money.\n\nSolarMaps automates the solar sales process so that property owners interested in going solar can self qualify and self refer themselves through an open, information rich, and social process.  SolarMaps also allows existing solar customers to share the real savings that they have received from their installations and automatically refer more people to their developer, creating an additional revenue stream for them, while they help more people understand the dollars and cents value of solar.\n\nAs a team!",
      "technologies": [
        "brains",
        "google-maps",
        "google-streetview",
        "google-sunroof",
        "python",
        "ruby-on-rails",
        "sketch"
      ]
    },
    {
      "project_name": "ChessEye",
      "likes": 10,
      "submitted_to": "TechCrunch Disrupt NY 2016",
      "winner": false,
      "created_by": "https://devpost.com/psuter",
      "description": "Computer chess without the computer\n\nComputers have revolutionized the practice of chess: not only can grandmasters analyze their games with unmatched precision, hundreds of thousands of amateurs can find opponents of their level online at any hour of the day. We wish to benefit from these incredible advances while preserving the joy of playing with a real chess set.\n\nChessEye is a computer vision system coupled with a chess-aware controller; it tracks chess pieces on regular, unmodified, chess boards and, because it understands the logic of the game, can follow along the moves as they are played. ChessEye also includes an artificial intelligence and can play against humans, or comment on their games. All feedback is given through speech synthesis. ChessEye can run and has been tested both on a commodity laptop and on a RaspberryPi.\n\nWe used OpenCV with its Python bindings to gather an estimate of the position on the board. This continuous scanning is fed into a ReactiveML controller, whose responsibility is to both eliminate noise and ensure that the moves are conforming to the rules of chess. The controller finally issues feedback sentences to the speech synthesis module which relies on the IBM Watson APIs to produce the sound samples.",
      "technologies": [
        "ocaml",
        "opencv",
        "python",
        "reactiveml"
      ]
    },
    {
      "project_name": "esusu",
      "likes": 10,
      "submitted_to": "TechCrunch Disrupt NY 2016",
      "winner": false,
      "created_by": "https://devpost.com/Weeniebaby6",
      "description": "Esusu, it is informal loan club, helping to save or have a short-term loan through the susu concept.\n\nOur team members live with student debt and have families who use alternative financial products outside of mainstream banking. We believe that crowd lending will help communities who no longer trust mainstream banks acquire credit, loans, and build savings.\n\neSuSu is a platform for friends, family members, colleagues, and neighbors to form groups to loan each other. Peers will be able to lend to each other and motivate each other to save through our platform. People who save in a group are more likely to succeed than those who do it alone. A platform will allow users to track their lending and financial transactions.\n\nWe used lean stack technologies and integrated the Office 365 into our API.",
      "technologies": [
        "angular.js",
        "domain.com",
        "express.js",
        "mongodb",
        "node.js",
        "office-365",
        "radix"
      ]
    },
    {
      "project_name": "AlexaWebBot",
      "likes": 6,
      "submitted_to": "TechCrunch Disrupt NY 2016",
      "winner": true,
      "created_by": "https://devpost.com/markoblad",
      "description": "Alexize your website with the help of natural language processing\n\nI want to order flowers for my mom for Mother's Day.  Straightforward.  I don't need to sit down, and I don't need to run though login on my phone.  I can give all the necessary information just by answering questions.  Apps have been mobilized.  They should also be \"Alexized.\"  We'd like to facilitate this by automatically generating the intents from typical workflows.\n\nTell your Amazon Echo what page you want to visit.  Using Amazon's Alexa, Amazon Lambda's are executed to call servers that have pre-processed website actions.  Voice commands are translated to actions that the server executes through a browser to navigate, browse, log in, and submit other actions to the website.\nIdeal website candidates are those that don't already have APIs, effectively turning these sites into a modern version.",
      "technologies": [
        "alexa",
        "amazon-alexa",
        "javascript",
        "node.js",
        "python",
        "radix",
        "ruby",
        "selenium"
      ]
    },
    {
      "project_name": "Salt and Pepper",
      "likes": 5,
      "submitted_to": "Junction Asia",
      "winner": false,
      "created_by": "https://devpost.com/Scub3d",
      "description": "No more worrying about your house when you are not home. We give you full control of Pepper with VR from anywhere.\n\nPeople worry about inside their house when they are not home. We want to give people the ability to not only see inside their home but also have the ability to control an avatar to complete tasks that was previously impossible unless they are physically there.\n\nWith VR goggles you have the ability to have a full-body take-over of Pepper and control its movements, arms, and vision. With this avatar, you can complete tasks at home using Pepper from anywhere in the world. From doing housework, to checking the stove, and even first emergency response, Pepper will be an extension of you while you are not present.\n\nTechs we used:\nPepper\nLeap Motion\nAndroid\nOculus\nUnity\nSalt",
      "technologies": []
    },
    {
      "project_name": "Handy",
      "likes": 11,
      "submitted_to": "Abilities Hackathon",
      "winner": false,
      "created_by": "https://devpost.com/michellemartir",
      "description": "A text to ASL interpreter. Available as a Chrome Extension and Android App\n\nMany in the deaf community prefer to watch someone signing instead of reading large portions of text in English. This makes sense, as ASL (American Sign Language) is their first language, and English may be their second. In cases where a deaf person was born outside of the United States, they may know ASL, but not English. In cases such as this, ASL signing is a necessity to communicate. However, most deaf people do have an interpreter with them at all times.\n\nWe built a chrome extension and Android App that allows a deaf user to highlight a block of text on a webpage, and translate it into ASL. The translation is done one word at a time, using video clips of an ASL interpreter who is signing those individual words.\n\nA backend server, running ruby-on-rails, accepts HTTP requests from a client. The request should contain the text that is to be translated. The server parses the text, one word at a time, and looks them up on the ASL dictionary site handspeak.com. The videos are pulled from handspeak.com and stitched together, and returned to the client as a single video.\nThe client can be anything that can send HTTP requests, and can play MP4 videos. For the hackathon, we create a chrome extension that allows a user to highlight any text on a webpage, right click it, and then click a \"Sign with Handy\" menu item. This will open a new web page with the generated video of the signed text. The Android App we developed works in a similar fashion. You can select any text from a web page or another app, click the share button, and then select \"Handy\". The Handy app will open and display the generated video of the signed text.",
      "technologies": [
        "android",
        "bash",
        "chrome",
        "css",
        "html",
        "java",
        "javascript",
        "ruby-on-rails",
        "sinatra",
        "spring"
      ]
    },
    {
      "project_name": "SpoilFoil",
      "likes": 6,
      "submitted_to": "SB Hacks II",
      "winner": false,
      "created_by": "https://devpost.com/canrobins13",
      "description": "A light chrome extension to block spoilers non-intrusively before they wreck your favorite show\n\nInput keywords that you'd like to avoid, and SpoilFoil will blur areas of text in your browser that might contain spoilers! At the click of a button, you can choose to unblur the text to reveal what's hidden below. SpoilFoil is non-intrusive and doesn't interfere with your browsing experience. It's lightweight, easy-to-use, and a must-have for any hardcore fan.\n\nBuilt on the Chrome extension API. Written in  standard JS with some JQuery.",
      "technologies": [
        "chrome",
        "css",
        "html5",
        "javascript",
        "jquery"
      ]
    },
    {
      "project_name": "CrimeWatch",
      "likes": 6,
      "submitted_to": "HackSussex2016",
      "winner": false,
      "created_by": "https://devpost.com/charlesworth",
      "description": "Pebble App to tell you the crime rates in your area\n\nWanted to make something cool with the Pebble and Police data.\n\nTakes a user's location and passes that to the Police.uk data API. That then returns a set of data about recent crimes in the area. This is displayed to the user in a variety of ways. For instance, you are able to see how many crimes, the type of crimes and their frequency and the exact spot that most of the crimes occurred.\nAdditionally users can shake the pebble to refetch data and can set the app to automatically update every 20mins if they are walking around and want regular updates.\n\nUsing the CloudPebble IDE and PebbleJS API (plus GitHub ofc).",
      "technologies": [
        "javascript"
      ]
    },
    {
      "project_name": "eySonos",
      "likes": 4,
      "submitted_to": "Abilities Hackathon",
      "winner": false,
      "created_by": "https://devpost.com/MaxCorbin",
      "description": "How to turn a person into a Roomba. Also, it glows in the dark.\n\nThe original inspiration came from an aunt with degrading vision.  This led to a simple question of if we can have cars auto-navigate, why can't we build hardware to help the blind navigate?  If we can, is it possible to do it cheaply?\n\nIt starts simple by using a scanning array of ultrasound sensors to provide aucostic feedback about what is seen in the environment.  In essence, using a similar system to those used by the common Roomba robots.",
      "technologies": [
        "arduino"
      ]
    },
    {
      "project_name": "autocolorization",
      "likes": 17,
      "submitted_to": "5C Hackathon Spring 2016",
      "winner": false,
      "created_by": "https://devpost.com/mbartoli",
      "description": "We colorized Jean-Luc Godard's Breathless (1960) using deep learning",
      "technologies": [
        "caffe",
        "docker",
        "ffmpeg",
        "lua",
        "python",
        "torch"
      ]
    },
    {
      "project_name": "Drone data logger with RF",
      "likes": 9,
      "submitted_to": "MenloHacks",
      "winner": false,
      "created_by": "https://devpost.com/Mas_Grebnesor",
      "description": "We built a system which collects weather data from altitudes by using different sensors and a home made drone.\n\nWe love the outdoors but sometimes the weather gets in the way. If only you knew what the weather was instantly and more accurate than a phone app.\n\nOur drone-mounted data logger can collect a wide variety of data; including humidity, temperature, pressure, altitude, and UV light radiation. It can attach to most remote controlled drones where you can fly it very high and have the data be beamed down instantly via radio.\n\nWe first bought our parts online. Then we programmed the Arduinos while simultaneously putting together the hardware. At the end, we coded a Mac application to display the data in a sleek and easy-to-use interface.",
      "technologies": [
        "arduino",
        "baseflight",
        "naze32",
        "processing",
        "serial",
        "sparkfun"
      ]
    },
    {
      "project_name": "stripr",
      "likes": 10,
      "submitted_to": "Uncommon Hacks Spring 2016",
      "winner": true,
      "created_by": "https://devpost.com/shankar2196",
      "description": "Redefine your power moves, an efficient and intelligent power strip\n\nPower strips today have practical problems that make them severely inefficient; put in two laptop chargers and suddenly a 6-plug strip can be completely used up. There are some variations on the market that attempt at a more flexible structure, yet those are still restrictive in design, expensive, difficult to mount, and don't respond to specific user needs, such as USB ports or international travel.  So we decided to build an efficient and intelligent power strip that has a more flexible design, provides usage information, and responds to wireless control.\n\nStructurally, the power strip, stripr, is divided into modules that can extend and rotate, giving more room for diverse plug designs.  An LCD display shows real time voltages across each module, and this information is also sent to the cloud where it can be analyzed and the results sent wirelessly to the user's computer or phone.\n\nThe structural parts of the power strip were built by 3D-printing ABS filament, and the modules are connected by adjustable metal rods which house the wires connecting the modules.  We then built a circuit connecting an Arduino with a LCD display, the system also serving as a digital multimeter that measures voltage drops.  The Arduino then sends the voltage numbers to a computing system where it can be stored as a text file in the cloud.\nFor the sake of the prototype and due to limitations in size, parts, and time, the Arduino, breadboards, and circuit wiring are attached to the power strip on the outside.",
      "technologies": [
        "3dprinting",
        "arduino",
        "autodesk",
        "inventor"
      ]
    },
    {
      "project_name": "1-657-BEER-BOT",
      "likes": 5,
      "submitted_to": "Brewhacks",
      "winner": false,
      "created_by": "https://devpost.com/mankins",
      "description": "I'm not sitting at the bar alone. I'm txt'ing and learning about with my pal: 1-657-BEER-BOT.\n\nI once heard that in ancient Rome people were able to wayfind by sampling the water. Apparently each well tasted slightly differently and after awhile you knew what part of town you were in by how metallic the water tasted.\nI wanted to do a modern version of this, but with the \"popular\" beer of a neighborhood. Only...I didn't know how to find out what the popular beer of a neighborhood is...so I decided to make my own data collection device.\nI don't know about you, but when I'm drinking a beer, the last thing I want is a computer in the way, sampling my beer preferences. As minutes turned into hours, I felt a nag pulling me toward a minimal interface that would help users as a bar-buddy might.\n1-657-BEER-BOT was born to accompany users on their crawls throughout the city. Users learn about the beers they're enjoying with a minimal interface that fades in and out of focus, much like a conversation with many people ... at a bar.\n\n1-657-BEER-BOT is a SMS + Camera + Web experience.\nUpload a photo of your beer bottle and not only will you get back a description of the beer, including food pairings, and alcohol content, but he'll also record the drink in a log.\nSend a and you'll get back the beers that are most popular in this area. (If you squint, you can see a bit of the initial idea here.)\nDon't know a beer on draft? Simply type in the name and you'll get expert guidance on its strength and flavor.\nSince 1-657-BEER-BOT doesn't drink, he's watching how much you've had. Send a  and he'll estimate your blood alcohol level.\nJust like a drinking buddy, 1-657-BEER-BOT will sometimes spout off trivia to keep you entertained.",
      "technologies": [
        "amazon-web-services",
        "brewery-db",
        "cloudsight",
        "google-vision",
        "lambda",
        "node.js",
        "sendgrid",
        "tiny-url",
        "twilio"
      ]
    },
    {
      "project_name": "Airena",
      "likes": 27,
      "submitted_to": "CarlHacks 2016",
      "winner": true,
      "created_by": "https://devpost.com/OmarShe7ta",
      "description": "A game where you write & edit AI code on the fly to fight in the arena!\n\nI've always loved the idea of writing an AI bot to challenge or fight others. I think it's a super fun way to learn to program and feels empowering. I always wanted to host a \"St. Olaf vs Carleton Bot Wars\" sort of thing, but none of the available tools really make it easy. That's mainly because:\nYou can't challenge individual people or groups. You can only upload your code to fight in the global leaderboards.\nYou can't easily play the game as a human, or play against your own bot as a human player.\nMost games are grid-based. (Which is ideal for developing AI, but is often not what the real world looks/feels like)\nSo I set out to make something that should be: simple, flexible and tangible. I wanted something that does not obscure the process of writing code. No one writes a perfect algorithm from scratch on their first try. Programming is an iterative process. It's very important for beginners to see that to understand how talented programmers do what they do and how they got to where they are. * My tool will be highlighting, rather than obscuring, the process of writing code.*\n\nAirena (pronounced Eye-reen-a) is a multiplayer game, a teaching tool and a (prototype of a) platform. It's a space shooter that you can play as a human. You can immediately click on the code button to write up your bot and watch it take over. Joining the game is as easy as opening the link.",
      "technologies": [
        "create.js",
        "node.js",
        "socket.io"
      ]
    },
    {
      "project_name": "got mesh?",
      "likes": 10,
      "submitted_to": "HackPSU - Spring 2016",
      "winner": false,
      "created_by": "https://devpost.com/gcb5083",
      "description": "The most cheap and simplistic open source 3D scanner, with no moving parts.\n\nSitting in an introductory first year engineering design class, we were assigned to create a digital model of a mug from a 3D rough scanned one. It was infuriating to have such an ambiguous model to recreate just from guessing at inaccurate dimensions. Therefore a solution was sought out to create high resolution scans with minimal complexity and cost. The solution found uses no motors to scan a side of an object in 3D. This is accomplished by just using any image taking time lapse device or app.\n\nThe device essentially drains a tub of liquid around a solid object of different color to produce multiple 2D traceable contours continuously until a series of \"sliced\" images are formed. An accompanying python script then takes the series of image slices, finds the edge boundary of the object to the fluid, extracts it into a 3D array and outputs the data into a standard point cloud file format.\n\nOur first design used a mechanical gearbox and linear actuator to control the relative fluid depth to make the depth based contours. Because we wanted to maintain simplicity, repeat-ability and low cost we switched our design. Our design is solely two tubs stacked on one another with a drain hole in the middle of them. This allows for the fluid to be drained from the top bucket to the bottom, without the use of motors. The only other part is a phone holder securely resting on top of the top bucket to take the time lapse. In the future, higher end DSLR time lapse cameras could be employed with better lighting",
      "technologies": [
        "opencv",
        "python"
      ]
    },
    {
      "project_name": "Money Accordion",
      "likes": 13,
      "submitted_to": "Bitcamp 2016",
      "winner": false,
      "created_by": "https://devpost.com/RobertAdkins",
      "description": "It's a dollar bill that you play like an accordion.\n\nPaper currency is so ubiquitous -- why can't we use it to express ourselves through music?\n\nA tactile way to create music by using items commonly found in the household to control computer-based synthesizers. The dollar bill makes a great accordion!\n\nOpenCV for computer vision recognition of dollar bill and fingers. Virtual MIDI port communication to open-sourced synthesizer Helm.",
      "technologies": [
        "helm",
        "opencv",
        "python",
        "rtmidi"
      ]
    },
    {
      "project_name": "Expo-Dyte",
      "likes": 6,
      "submitted_to": "Bitcamp 2016",
      "winner": false,
      "created_by": "https://devpost.com/KyleHaptonstall",
      "description": "Streamlined Hackathon Judging\n\nGiving Hackathon judges a streamlines system to rate projects during an expo, as well as ensuring each table gets equal facetime with judges.\n\nWe deliver an interface similar to Devpost's web layout that allows judges to easily signin to a Hackathon and view all submissions made under specific categories so they can judge straight from our mobile app.\nAlong with our mobile app, we have a functioning Devpost web scraper and API to which all calls are made within the app.\n\nThe app is a native iOS app built using Swift. Our backend is powered by Node.js and uses Express as a web server.",
      "technologies": [
        "css",
        "express.js",
        "html",
        "javascript",
        "mongodb",
        "node.js",
        "swift"
      ]
    },
    {
      "project_name": "Alexa JARVIS",
      "likes": 33,
      "submitted_to": "WearHacks Kitchener Waterloo 2016",
      "winner": true,
      "created_by": "https://devpost.com/mdimic",
      "description": "A modular voice controlled helmet attachment.\n\nWith the ever growing danger of distracted driving related incidents, our goal was to increase safety for motorcyclists and prevent dangerous accidents, while still maintaining a good user experience.\n\nAlexa JARVIS is an intuitive voice controlled smart helmet. The goal was to create a completely voice controlled interactive system to assist a motorcyclist on the road, while leaving the other four senses to focus on the safety of the user. This is different than other smart helmets on the market because it does not impede the sense of sight, it is easily accessible and can be adapted to many different helmets, and it is a cheaper alternative to competitors.\nSome examples of the commands you can run are:\n“Navigate home”\n“Navigate to work”\n“Where am I?”\n“How long will it take me to get to work?”\n“Text Mike I’ll be there in 5 minutes”\n“Send my location to mom”\n“Help” – Sends out a distress signal with your location to a list of emergency contacts\n\nWe incorporated Amazon’s Alexa voice service into a Raspberry Pi 2 and attached it to the helmet. We built the Alexa skill to communicate with the rider, using node.js and firebase for the backend. The Raspberry Pi is powered by an external USB battery pack and the electroluminescent lights are controlled using a relay which is controlled by the Raspberry Pi.",
      "technologies": [
        "amazon-alexa",
        "angular.js",
        "apache",
        "firebase",
        "google-directions",
        "lambda",
        "node.js",
        "python",
        "raspberry-pi",
        "relay",
        "twilio"
      ]
    },
    {
      "project_name": "AR Watch",
      "likes": 23,
      "submitted_to": "WearHacks Kitchener Waterloo 2016",
      "winner": true,
      "created_by": "https://devpost.com/iYung",
      "description": "An augmented reality wearable with Google Cardboard and Vuforia\n\nWe play video games like Fallout and Mass Effect. Both games have wrist devices which display data. The team decided to try to mimic it.\n\nAR Watch acts as a digital watch through the Google Cardboard. Four functions can be selected by the user while one functions runs at all times. The first function tells the time like a normal watch. The second function is a weather function which tells the current temperature for the city the AR Watch is in. The third function tells the date like a calendar. The last user selected function is to display a Bing map centered on the user's location. The function which runs at all times is the battery indicator.\n\nThe AR Watch is built in Unity with Vuforia and Google Cardboard SDKs. Unity is the main platform where everything is put together. Vuforia is used to detect a image which determines where Unity displays the AR Watch. In order to display data across the watch faces, AR Watch made REST calls to various APIs. The map data came from Bing while the weather data came from OpenWeatherMap.",
      "technologies": [
        "c#",
        "google-cardboard",
        "unity",
        "vuforia"
      ]
    },
    {
      "project_name": "QuikCaption",
      "likes": 7,
      "submitted_to": "Grizzhacks",
      "winner": false,
      "created_by": "https://devpost.com/tizz98",
      "description": "Why type hashtags when you can select them from a list based off of your picture?\n\nIt takes a long time to type the specific hashtags for your image social media post, so why not make it simpler with suggestions?\n\nQuikCaption takes the photo you select or take and then gets some suggested hashtags that you can select and then post to your social media.\n\nWe used the amazing Clarifai api to generate the tags. For the iOS app we used Objective-C and for the Android app we used Java.",
      "technologies": [
        "android",
        "clarifai",
        "html",
        "instagram",
        "ios",
        "java",
        "objective-c",
        "twitter"
      ]
    },
    {
      "project_name": "Secret Set",
      "likes": 7,
      "submitted_to": "SXSW Music Hackathon",
      "winner": true,
      "created_by": "https://devpost.com/RBdevelop",
      "description": "A place where artists can create exclusive sets for their fans.\n\nI want more intimate experiences with my favorite artists! Is that too much to ask? Little creepy yes, but how awesome would it be to have an actual conversation with them!\n\nArtists create a \"secret set\" which includes when the set goes live, what content they want to include (audio, video, live-stream), and how to structure the set (free for first 1000, $.99 after). They then send out a special link/code through their social media platforms for people to use to connect to the secret set that a specific time. Once connected consumers can interact with the artist, listen, view, and enjoy with like minded people and the artist themselves. When the artists leaves the set is over.\n\nReact Native mobile app deployed to iOS, built on-top of Firebase for real-time messaging and Twilio SMS for auth.",
      "technologies": [
        "react",
        "react-native"
      ]
    }
  ]
}